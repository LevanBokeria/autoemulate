{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the input and output of the simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the path to the folder of the outputs and the samples.csv\n",
    "The outputs here are generated using ModularCirc Library (Tutorial_03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = '/Users/mfamili/work/ModularCirc/Tutorials/Tutorial_03/Outputs/Out_01'\n",
    "in_path = '/Users/mfamili/work/ModularCirc/Tutorials/Tutorial_03/samples_Naghavi.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = pd.read_csv(in_path)\n",
    "len(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read and process outputs\n",
    "- Read the outputs and convert them to a list of dictionaries. report if any output files are missing\n",
    "- convert outputs to a numpy array of (number of samples, number of outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "num_sim = 1000\n",
    "no_sim_result_list = []\n",
    "# Loop through the expected range of files\n",
    "for i in range(num_sim):\n",
    "    filename = f\"all_outputs_{i}.csv\"\n",
    "    file_path = os.path.join(out_path, filename)\n",
    "    \n",
    "    if os.path.exists(file_path):  # Check if the file exists\n",
    "        try:\n",
    "            # Read the DataFrame and append it to the list\n",
    "            df = pd.read_csv(file_path)\n",
    "            outputs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            outputs.append(False)  # Append False if there's an error\n",
    "    else:\n",
    "        no_sim_result_list.append(i)\n",
    "        print(f\"File {file_path} doesn't exist\")\n",
    "        outputs.append(False)  # Append False if the file doesn't exist\n",
    "\n",
    "outs = np.array([df.max().values - df.min().values for df in outputs])\n",
    "print(np.array(outs).shape)\n",
    "print(np.array(outs).dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update the parameters to drop the parameter sets where the output is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_parameters = parameters.drop(no_sim_result_list)\n",
    "updated_parameters= updated_parameters.to_numpy()\n",
    "updated_parameters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_methods = [\n",
    "    {\"name\": \"PCA\", \"params\": {\"reduced_dim\": 8}},\n",
    "    {\n",
    "        \"name\": \"VAE\",\n",
    "        \"params\": {\"reduced_dim\": 3, \"hidden_layers\": [64, 32], \"epochs\": 100},\n",
    "    },]\n",
    "#    {\"name\": \"None\", \"params\": {}},]\n",
    "\n",
    "\n",
    "\n",
    "#preprocessing_methods = [{\"name\": \"PCA\", \"params\": {\"n_components\": 8}}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup autoemulate \n",
    "- here we are choosing to preprocess the outputs in order to reduce the dimentionality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.compare import AutoEmulate\n",
    "em = AutoEmulate()\n",
    "em.setup(updated_parameters, outs, models=[\"gp\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = em.compare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.preprocessing_results.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.summarise_cv(model='gp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.summarise_cv()  # Skip None models by filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.evaluate(em.get_model()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.plot_eval(em.get_model(), input_index=[0,1,2,3,4], output_index=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.plot_cv(style=\"residual_vs_predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModularCirc.Models.NaghaviModel import NaghaviModel, NaghaviModelParameters, TEMPLATE_TIME_SETUP_DICT\n",
    "from ModularCirc import BatchRunner\n",
    "import os\n",
    "br = BatchRunner('LHS', 0)\n",
    "br.setup_sampler('parameters_01.json')\n",
    "br.sample(1)\n",
    "map_ = {\n",
    "    'lv.t_tr' : ['lv.t_tr',],\n",
    "    'la.t_tr' : ['la.t_tr',],\n",
    "    'la.delay' : ['la.delay',],\n",
    "    'lv.tau' : ['lv.tau',],\n",
    "    'la.tau' : ['la.tau',],\n",
    "    'lv.t_max' : ['lv.t_max',],\n",
    "    'la.t_max' : ['la.t_max',],\n",
    "}\n",
    "br.map_sample_timings(\n",
    "    ref_time=1000.,\n",
    "    map=map_\n",
    "    )\n",
    "br.samples\n",
    "br.samples.to_csv('samples_Naghavi.csv', index=False)\n",
    "br.map_vessel_volume()\n",
    "br._samples[['ao.v', 'art.v', 'ven.v']].describe().T\n",
    "br.setup_model(model=NaghaviModel, po=NaghaviModelParameters, time_setup=TEMPLATE_TIME_SETUP_DICT)\n",
    "test = br.run_batch(n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import uniform\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "# Define our simulator\n",
    "def simulator(params, noise=0.0):\n",
    "    \"\"\"\n",
    "    A simple simulator that takes parameters and returns outputs.\n",
    "    In real applications, this would be a complex numerical model.\n",
    "    \n",
    "    params: array of parameter values [x1, x2]\n",
    "    noise: noise level to add to the output\n",
    "    \"\"\"\n",
    "    x1, x2 = params\n",
    "    # Nonlinear function as our simulator\n",
    "    y = np.sin(x1) * np.cos(x2) + x1 * x2 / 5.0\n",
    "    \n",
    "    # Add some noise if specified\n",
    "    if noise > 0:\n",
    "        y += np.random.normal(0, noise)\n",
    "    \n",
    "    return y\n",
    "\n",
    "# Generate \"observed\" historical data\n",
    "# In a real application, this would be actual measurements\n",
    "np.random.seed(42)\n",
    "true_params = [2.5, 1.8]  # True parameter values that generated the historical data\n",
    "observed_data = simulator(true_params, noise=0.2)\n",
    "print(f\"Observed historical data: {observed_data}\")\n",
    "\n",
    "# Define parameter space for sampling\n",
    "param_ranges = [(0, 5), (0, 5)]  # Ranges for parameters x1 and x2\n",
    "\n",
    "# Latin Hypercube Sampling to explore the parameter space\n",
    "# For simplicity, we'll use random sampling instead\n",
    "n_samples = 100\n",
    "samples = np.array([\n",
    "    [uniform.rvs(param_ranges[0][0], param_ranges[0][1]-param_ranges[0][0]) for _ in range(n_samples)],\n",
    "    [uniform.rvs(param_ranges[1][0], param_ranges[1][1]-param_ranges[1][0]) for _ in range(n_samples)]\n",
    "]).T\n",
    "\n",
    "# Run the simulator for each sample\n",
    "simulator_outputs = np.array([simulator(params) for params in samples])\n",
    "\n",
    "# Calculate the implausibility metric\n",
    "# This measures how far simulator outputs are from observed data\n",
    "# normalized by the uncertainty\n",
    "def implausibility(sim_output, obs_data, obs_error=0.2, model_error=0.1):\n",
    "    \"\"\"\n",
    "    Calculate implausibility metric\n",
    "    \n",
    "    sim_output: simulator output\n",
    "    obs_data: observed data\n",
    "    obs_error: observation error standard deviation\n",
    "    model_error: model discrepancy standard deviation\n",
    "    \"\"\"\n",
    "    total_variance = obs_error**2 + model_error**2\n",
    "    return np.abs(sim_output - obs_data) / np.sqrt(total_variance)\n",
    "\n",
    "# Calculate implausibility for each sample\n",
    "implausibility_scores = implausibility(simulator_outputs, observed_data)\n",
    "\n",
    "# Build a Gaussian Process emulator\n",
    "# This will help us predict simulator outputs for new parameter values\n",
    "kernel = C(1.0) * RBF(length_scale=[1.0, 1.0])\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
    "gp.fit(samples, simulator_outputs)\n",
    "\n",
    "# Function to predict simulator output using GP emulator\n",
    "def emulate(params):\n",
    "    \"\"\"Use GP emulator to predict simulator output\"\"\"\n",
    "    return gp.predict([params])[0]\n",
    "\n",
    "# Function to calculate emulated implausibility\n",
    "def emulated_implausibility(params, obs_data, obs_error=0.2, model_error=0.1, emulator_error=0.1):\n",
    "    \"\"\"Calculate implausibility using the emulator\"\"\"\n",
    "    emulated_output = emulate(params)\n",
    "    total_variance = obs_error**2 + model_error**2 + emulator_error**2\n",
    "    return np.abs(emulated_output - obs_data) / np.sqrt(total_variance)\n",
    "\n",
    "# Create a grid of parameter values to evaluate the emulator\n",
    "n_grid = 50\n",
    "x1_grid = np.linspace(param_ranges[0][0], param_ranges[0][1], n_grid)\n",
    "x2_grid = np.linspace(param_ranges[1][0], param_ranges[1][1], n_grid)\n",
    "X1, X2 = np.meshgrid(x1_grid, x2_grid)\n",
    "\n",
    "# Calculate implausibility for each grid point\n",
    "Z = np.zeros((n_grid, n_grid))\n",
    "for i in range(n_grid):\n",
    "    for j in range(n_grid):\n",
    "        params = [X1[i, j], X2[i, j]]\n",
    "        Z[i, j] = emulated_implausibility(params, observed_data)\n",
    "\n",
    "# Define a cutoff for implausibility\n",
    "# Points with implausibility below this are considered \"not implausible\"\n",
    "implausibility_cutoff = 3.0\n",
    "\n",
    "# Find the parameter sets that are not ruled out (not implausible)\n",
    "not_implausible_mask = Z < implausibility_cutoff\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot the implausibility surface\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.contourf(X1, X2, Z, levels=20, cmap='viridis_r')\n",
    "plt.colorbar(label='Implausibility')\n",
    "plt.scatter(samples[:, 0], samples[:, 1], c='white', s=10, alpha=0.5)\n",
    "plt.scatter(true_params[0], true_params[1], c='red', s=50, marker='*')\n",
    "plt.xlabel('Parameter x1')\n",
    "plt.ylabel('Parameter x2')\n",
    "plt.title('Implausibility Surface')\n",
    "\n",
    "# Plot the not implausible region\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.contourf(X1, X2, not_implausible_mask.astype(float), levels=1, cmap='Blues')\n",
    "plt.scatter(samples[:, 0], samples[:, 1], c='black', s=10, alpha=0.5)\n",
    "plt.scatter(true_params[0], true_params[1], c='red', s=50, marker='*')\n",
    "plt.xlabel('Parameter x1')\n",
    "plt.ylabel('Parameter x2')\n",
    "plt.title('Not Implausible Region')\n",
    "\n",
    "# Plot sample points colored by implausibility\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(samples[:, 0], samples[:, 1], c=implausibility_scores, cmap='viridis_r')\n",
    "plt.colorbar(label='Implausibility')\n",
    "plt.scatter(true_params[0], true_params[1], c='red', s=50, marker='*')\n",
    "plt.xlabel('Parameter x1')\n",
    "plt.ylabel('Parameter x2')\n",
    "plt.title('Sample Points')\n",
    "\n",
    "# Plot emulator predictions vs simulator outputs\n",
    "emulator_predictions = gp.predict(samples)\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(simulator_outputs, emulator_predictions)\n",
    "plt.plot([min(simulator_outputs), max(simulator_outputs)], \n",
    "         [min(simulator_outputs), max(simulator_outputs)], 'k--')\n",
    "plt.xlabel('Simulator Output')\n",
    "plt.ylabel('Emulator Prediction')\n",
    "plt.title('Emulator vs Simulator')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find the parameter set with the lowest implausibility\n",
    "best_idx = np.unravel_index(np.argmin(Z), Z.shape)\n",
    "best_params = [X1[best_idx], X2[best_idx]]\n",
    "best_implausibility = Z[best_idx]\n",
    "\n",
    "print(f\"True parameters: {true_params}\")\n",
    "print(f\"Best matching parameters: {best_params}\")\n",
    "print(f\"Best implausibility: {best_implausibility}\")\n",
    "print(f\"Simulator output at best parameters: {simulator(best_params)}\")\n",
    "print(f\"Observed data: {observed_data}\")\n",
    "\n",
    "# Perform a history matching wave\n",
    "# In a real application, you would perform multiple waves\n",
    "print(\"\\nPerforming a second wave of history matching...\")\n",
    "\n",
    "# Use the not implausible points as the basis for a new wave\n",
    "not_implausible_points = []\n",
    "for i in range(n_grid):\n",
    "    for j in range(n_grid):\n",
    "        if not_implausible_mask[i, j]:\n",
    "            not_implausible_points.append([X1[i, j], X2[i, j]])\n",
    "\n",
    "# Sample from the not implausible region\n",
    "if len(not_implausible_points) > 0:\n",
    "    not_implausible_points = np.array(not_implausible_points)\n",
    "    \n",
    "    # Take a random sample from the not implausible points\n",
    "    wave2_samples_idx = np.random.choice(len(not_implausible_points), \n",
    "                                         min(100, len(not_implausible_points)), \n",
    "                                         replace=False)\n",
    "    wave2_samples = not_implausible_points[wave2_samples_idx]\n",
    "    \n",
    "    # Run the simulator for wave 2 samples\n",
    "    wave2_outputs = np.array([simulator(params) for params in wave2_samples])\n",
    "    \n",
    "    # Build a new GP emulator for wave 2\n",
    "    wave2_gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
    "    wave2_gp.fit(wave2_samples, wave2_outputs)\n",
    "    \n",
    "    # Calculate implausibility for wave 2\n",
    "    wave2_implausibility = implausibility(wave2_outputs, observed_data)\n",
    "    \n",
    "    # Find the parameter set with the lowest implausibility in wave 2\n",
    "    best_wave2_idx = np.argmin(wave2_implausibility)\n",
    "    best_wave2_params = wave2_samples[best_wave2_idx]\n",
    "    best_wave2_implausibility = wave2_implausibility[best_wave2_idx]\n",
    "    \n",
    "    print(f\"Wave 2 best parameters: {best_wave2_params}\")\n",
    "    print(f\"Wave 2 best implausibility: {best_wave2_implausibility}\")\n",
    "    print(f\"Wave 2 simulator output: {simulator(best_wave2_params)}\")\n",
    "    print(f\"Observed data: {observed_data}\")\n",
    "    \n",
    "    # Plot wave 2 results\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(wave2_samples[:, 0], wave2_samples[:, 1], c=wave2_implausibility, cmap='viridis_r')\n",
    "    plt.colorbar(label='Implausibility')\n",
    "    plt.scatter(true_params[0], true_params[1], c='red', s=100, marker='*', label='True Parameters')\n",
    "    plt.scatter(best_wave2_params[0], best_wave2_params[1], c='green', s=100, marker='x', label='Best Match')\n",
    "    plt.xlabel('Parameter x1')\n",
    "    plt.ylabel('Parameter x2')\n",
    "    plt.title('Wave 2 History Matching')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No not implausible points found for wave 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

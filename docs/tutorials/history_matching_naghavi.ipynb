{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naghavi Model parameters set: \n",
      " * Component - \u001b[1m\u001b[93mao\u001b[0m\n",
      "  - \u001b[1m\u001b[93mr\u001b[0m       : 2.400e+02\n",
      "  - \u001b[1m\u001b[93mc\u001b[0m       : 3.000e-01\n",
      "  - \u001b[1m\u001b[93ml\u001b[0m       : 0.000e+00\n",
      "  - \u001b[1m\u001b[93mv_ref\u001b[0m   : 1.000e+02\n",
      "  - \u001b[1m\u001b[93mv\u001b[0m       : 1.300e+02\n",
      "  - \u001b[1m\u001b[93mp\u001b[0m       : nan\n",
      "\n",
      " * Component - \u001b[1m\u001b[93mart\u001b[0m\n",
      "  - \u001b[1m\u001b[93mr\u001b[0m       : 1.125e+03\n",
      "  - \u001b[1m\u001b[93mc\u001b[0m       : 3.000e+00\n",
      "  - \u001b[1m\u001b[93ml\u001b[0m       : 0.000e+00\n",
      "  - \u001b[1m\u001b[93mv_ref\u001b[0m   : 9.000e+02\n",
      "  - \u001b[1m\u001b[93mv\u001b[0m       : 1.092e+03\n",
      "  - \u001b[1m\u001b[93mp\u001b[0m       : nan\n",
      "\n",
      " * Component - \u001b[1m\u001b[93mven\u001b[0m\n",
      "  - \u001b[1m\u001b[93mr\u001b[0m       : 9.000e+00\n",
      "  - \u001b[1m\u001b[93mc\u001b[0m       : 1.333e+02\n",
      "  - \u001b[1m\u001b[93ml\u001b[0m       : 0.000e+00\n",
      "  - \u001b[1m\u001b[93mv_ref\u001b[0m   : 2.800e+03\n",
      "  - \u001b[1m\u001b[93mv\u001b[0m       : 3.780e+03\n",
      "  - \u001b[1m\u001b[93mp\u001b[0m       : nan\n",
      "\n",
      " * Component - \u001b[1m\u001b[93mav\u001b[0m\n",
      "  - \u001b[1m\u001b[93mr\u001b[0m       : 6.000e+00\n",
      "  - \u001b[1m\u001b[93mmax_func\u001b[0m : <function relu_max at 0x17aad0fe0>\n",
      "\n",
      " * Component - \u001b[1m\u001b[93mmv\u001b[0m\n",
      "  - \u001b[1m\u001b[93mr\u001b[0m       : 4.100e+00\n",
      "  - \u001b[1m\u001b[93mmax_func\u001b[0m : <function relu_max at 0x17aad0fe0>\n",
      "\n",
      " * Component - \u001b[1m\u001b[93mla\u001b[0m\n",
      "  - \u001b[1m\u001b[93mE_pas\u001b[0m   : 4.400e-01\n",
      "  - \u001b[1m\u001b[93mE_act\u001b[0m   : 4.500e-01\n",
      "  - \u001b[1m\u001b[93mv_ref\u001b[0m   : 1.000e+01\n",
      "  - \u001b[1m\u001b[93mk_pas\u001b[0m   : 5.000e-02\n",
      "  - \u001b[1m\u001b[93mactivation_function\u001b[0m : <function activation_function_1 at 0x17aad16c0>\n",
      "  - \u001b[1m\u001b[93mt_tr\u001b[0m    : 2.250e+02\n",
      "  - \u001b[1m\u001b[93mt_max\u001b[0m   : 1.500e+02\n",
      "  - \u001b[1m\u001b[93mtau\u001b[0m     : 2.500e+01\n",
      "  - \u001b[1m\u001b[93mdelay\u001b[0m   : 1.000e+02\n",
      "  - \u001b[1m\u001b[93mv\u001b[0m       : 9.360e+01\n",
      "  - \u001b[1m\u001b[93mp\u001b[0m       : nan\n",
      "  - \u001b[1m\u001b[93mactivation_func\u001b[0m : nan\n",
      "\n",
      " * Component - \u001b[1m\u001b[93mlv\u001b[0m\n",
      "  - \u001b[1m\u001b[93mE_pas\u001b[0m   : 1.000e+00\n",
      "  - \u001b[1m\u001b[93mE_act\u001b[0m   : 3.000e+00\n",
      "  - \u001b[1m\u001b[93mv_ref\u001b[0m   : 1.000e+01\n",
      "  - \u001b[1m\u001b[93mk_pas\u001b[0m   : 2.700e-02\n",
      "  - \u001b[1m\u001b[93mactivation_function\u001b[0m : <function activation_function_1 at 0x17aad16c0>\n",
      "  - \u001b[1m\u001b[93mt_tr\u001b[0m    : 4.200e+02\n",
      "  - \u001b[1m\u001b[93mt_max\u001b[0m   : 2.800e+02\n",
      "  - \u001b[1m\u001b[93mtau\u001b[0m     : 2.500e+01\n",
      "  - \u001b[1m\u001b[93mdelay\u001b[0m   : nan\n",
      "  - \u001b[1m\u001b[93mv\u001b[0m       : 1.040e+02\n",
      "  - \u001b[1m\u001b[93mp\u001b[0m       : nan\n",
      "  - \u001b[1m\u001b[93mactivation_func\u001b[0m : nan\n",
      "\n",
      "\n",
      " -- Variable \u001b[1m\u001b[93mv_ao\u001b[0m added to the principal variable key list.\n",
      "    - name of update function: \u001b[1m\u001b[93mchamber_volume_rate_change\u001b[0m\n",
      "    - inputs: ['q_av', 'q_ao']\n",
      " -- Variable \u001b[1m\u001b[93mv_art\u001b[0m added to the principal variable key list.\n",
      "    - name of update function: \u001b[1m\u001b[93mchamber_volume_rate_change\u001b[0m\n",
      "    - inputs: ['q_ao', 'q_art']\n",
      " -- Variable \u001b[1m\u001b[93mv_ven\u001b[0m added to the principal variable key list.\n",
      "    - name of update function: \u001b[1m\u001b[93mchamber_volume_rate_change\u001b[0m\n",
      "    - inputs: ['q_art', 'q_ven']\n",
      " -- Variable \u001b[1m\u001b[93mv_la\u001b[0m added to the principal variable key list.\n",
      "    - name of update function: \u001b[1m\u001b[93mchamber_volume_rate_change\u001b[0m\n",
      "    - inputs: ['q_ven', 'q_mv']\n",
      " -- Variable \u001b[1m\u001b[93mv_lv\u001b[0m added to the principal variable key list.\n",
      "    - name of update function: \u001b[1m\u001b[93mchamber_volume_rate_change\u001b[0m\n",
      "    - inputs: ['q_mv', 'q_av']\n",
      " -- Variable \u001b[1m\u001b[93mp_lv\u001b[0m added to the init list.\n",
      "    - name of update function: \u001b[1m\u001b[93mtotal_p\u001b[0m\n",
      "    - inputs: ['v_lv']\n",
      " -- Variable \u001b[1m\u001b[93mp_lv\u001b[0m added to the principal variable key list.\n",
      "    - name of update function: \u001b[1m\u001b[93mtotal_dpdt\u001b[0m\n",
      "    - inputs: ['v_lv', 'q_mv', 'q_av']\n",
      " -- Variable \u001b[1m\u001b[93mq_av\u001b[0m added to the secondary variable key list.\n",
      "    - name of update function: \u001b[1m\u001b[93mnon_ideal_diode_flow + max_func\u001b[0m\n",
      "    - inputs: ['p_lv', 'p_ao']\n",
      " -- Variable \u001b[1m\u001b[93mp_ao\u001b[0m added to the init list.\n",
      "    - name of update function: \u001b[1m\u001b[93mgrounded_capacitor_model_pressure\u001b[0m\n",
      "    - inputs: ['v_ao']\n",
      " -- Variable \u001b[1m\u001b[93mp_ao\u001b[0m added to the principal variable key list.\n",
      "    - name of update function: \u001b[1m\u001b[93mgrounded_capacitor_model_dpdt\u001b[0m\n",
      "    - inputs: ['q_av', 'q_ao']\n",
      " -- Variable \u001b[1m\u001b[93mp_art\u001b[0m added to the init list.\n",
      "    - name of update function: \u001b[1m\u001b[93mgrounded_capacitor_model_pressure\u001b[0m\n",
      "    - inputs: ['v_art']\n",
      " -- Variable \u001b[1m\u001b[93mp_art\u001b[0m added to the principal variable key list.\n",
      "    - name of update function: \u001b[1m\u001b[93mgrounded_capacitor_model_dpdt\u001b[0m\n",
      "    - inputs: ['q_ao', 'q_art']\n",
      " -- Variable \u001b[1m\u001b[93mq_ao\u001b[0m added to the secondary variable key list.\n",
      "    - name of update function: \u001b[1m\u001b[93mresistor_model_flow\u001b[0m\n",
      "    - inputs: ['p_ao', 'p_art']\n",
      " -- Variable \u001b[1m\u001b[93mp_ven\u001b[0m added to the init list.\n",
      "    - name of update function: \u001b[1m\u001b[93mgrounded_capacitor_model_pressure\u001b[0m\n",
      "    - inputs: ['v_ven']\n",
      " -- Variable \u001b[1m\u001b[93mp_ven\u001b[0m added to the principal variable key list.\n",
      "    - name of update function: \u001b[1m\u001b[93mgrounded_capacitor_model_dpdt\u001b[0m\n",
      "    - inputs: ['q_art', 'q_ven']\n",
      " -- Variable \u001b[1m\u001b[93mq_art\u001b[0m added to the secondary variable key list.\n",
      "    - name of update function: \u001b[1m\u001b[93mresistor_model_flow\u001b[0m\n",
      "    - inputs: ['p_art', 'p_ven']\n",
      " -- Variable \u001b[1m\u001b[93mp_la\u001b[0m added to the init list.\n",
      "    - name of update function: \u001b[1m\u001b[93mtotal_p\u001b[0m\n",
      "    - inputs: ['v_la']\n",
      " -- Variable \u001b[1m\u001b[93mp_la\u001b[0m added to the principal variable key list.\n",
      "    - name of update function: \u001b[1m\u001b[93mtotal_dpdt\u001b[0m\n",
      "    - inputs: ['v_la', 'q_ven', 'q_mv']\n",
      " -- Variable \u001b[1m\u001b[93mq_ven\u001b[0m added to the secondary variable key list.\n",
      "    - name of update function: \u001b[1m\u001b[93mresistor_model_flow\u001b[0m\n",
      "    - inputs: ['p_ven', 'p_la']\n",
      " -- Variable \u001b[1m\u001b[93mq_mv\u001b[0m added to the secondary variable key list.\n",
      "    - name of update function: \u001b[1m\u001b[93mnon_ideal_diode_flow + max_func\u001b[0m\n",
      "    - inputs: ['p_la', 'p_lv']\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from ModularCirc.Models.NaghaviModel import NaghaviModel, NaghaviModelParameters\n",
    "from ModularCirc.Solver import Solver\n",
    "TEMPLATE_TIME_SETUP_DICT = {\n",
    "    'name'       :  'TimeTest',\n",
    "    'ncycles'    :  40,\n",
    "    'tcycle'     :  1.0,\n",
    "    'dt'         :  0.001, \n",
    "    'export_min' :  1\n",
    " }\n",
    "parobj = NaghaviModelParameters()\n",
    "model = NaghaviModel(time_setup_dict=TEMPLATE_TIME_SETUP_DICT, parobj=parobj)\n",
    "solver = Solver(model=model)\n",
    "solver.setup()\n",
    "v_lv = solver.model.components['lv'].V.values\n",
    "p_lv = solver.model.components['lv'].P_i.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HistoryMatching' from 'autoemulate.history_matching' (/Users/mfamili/work/autoemulate/autoemulate/history_matching.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Tuple, List\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mautoemulate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhistory_matching\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HistoryMatching\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mNaghaviHistoryMatcher\u001b[39;00m:\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, observed_data: Dict[\u001b[38;5;28mstr\u001b[39m, np.ndarray], \n\u001b[32m     19\u001b[39m                 param_ranges: Dict[\u001b[38;5;28mstr\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]],\n\u001b[32m     20\u001b[39m                 n_cycles: \u001b[38;5;28mint\u001b[39m = \u001b[32m40\u001b[39m, dt: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.001\u001b[39m,\n\u001b[32m     21\u001b[39m                 implausibility_threshold: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m3.0\u001b[39m,\n\u001b[32m     22\u001b[39m                 model_discrepancy: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.0\u001b[39m):\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'HistoryMatching' from 'autoemulate.history_matching' (/Users/mfamili/work/autoemulate/autoemulate/history_matching.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import uniform\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from ModularCirc.Models.NaghaviModel import NaghaviModel, NaghaviModelParameters\n",
    "from ModularCirc.Analysis.BaseAnalysis import BaseAnalysis\n",
    "from ModularCirc.Solver import Solver\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Tuple, List\n",
    "from autoemulate.history_matching import HistoryMatching\n",
    "\n",
    "\n",
    "class NaghaviHistoryMatcher:\n",
    "    def __init__(self, observed_data: Dict[str, np.ndarray], \n",
    "                param_ranges: Dict[str, Tuple[float, float]],\n",
    "                n_cycles: int = 40, dt: float = 0.001,\n",
    "                implausibility_threshold: float = 3.0,\n",
    "                model_discrepancy: float = 0.0):\n",
    "        \"\"\"\n",
    "        Initialize the history matching framework for Naghavi model\n",
    "        \n",
    "        Args:\n",
    "            observed_data: Dictionary containing observed signals (keys: signal names, values: arrays)\n",
    "            param_ranges: Dictionary of parameter ranges to explore (keys: parameter names, values: (min, max))\n",
    "            n_cycles: Number of cardiac cycles to simulate\n",
    "            dt: Time step for simulation\n",
    "            implausibility_threshold: Threshold for considering points implausible\n",
    "            model_discrepancy: Model discrepancy term\n",
    "        \"\"\"\n",
    "        self.observed_data = observed_data\n",
    "        self.param_ranges = param_ranges\n",
    "        self.param_names = list(param_ranges.keys())\n",
    "        self.n_cycles = n_cycles\n",
    "        self.dt = dt\n",
    "        self.samples = None\n",
    "        self.simulator_outputs = None\n",
    "        self.gp_emulators = {}\n",
    "        \n",
    "        # Initialize the HistoryMatching object\n",
    "        self.history_matcher = HistoryMatching(\n",
    "            threshold=implausibility_threshold,\n",
    "            discrepancy=model_discrepancy,\n",
    "            rank=1\n",
    "        )\n",
    "        \n",
    "        # Time setup dictionary\n",
    "        self.time_setup = {\n",
    "            \"name\": \"HistoryMatching\",\n",
    "            \"ncycles\": n_cycles,\n",
    "            \"tcycle\": 1.0,  # Will be adjusted per simulation\n",
    "            \"dt\": dt,\n",
    "            \"export_min\": 1\n",
    "        }\n",
    "        \n",
    "        \n",
    "    def signal_get_pulse(self, signal: np.ndarray, dt: float, num: int = 100) -> Tuple[float, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Resample raw signal to standard resolution\n",
    "        \n",
    "        Args:\n",
    "            signal: Raw signal array\n",
    "            dt: Original time resolution\n",
    "            num: Number of points in resampled signal\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (new_dt, resampled_signal)\n",
    "        \"\"\"\n",
    "        ind = np.argmin(signal)        \n",
    "        ncycle = len(signal)\n",
    "        new_signal = np.interp(np.linspace(0, ncycle, num), \n",
    "                            np.arange(ncycle), \n",
    "                            np.roll(signal, -ind))\n",
    "        new_dt = ncycle / (num - 1) * dt\n",
    "        return new_dt, new_signal\n",
    "    \n",
    "    def run_simulation(self, params: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Run a single Naghavi model simulation with given parameters\n",
    "        \n",
    "        Args:\n",
    "            params: Dictionary of parameter values\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of processed features from simulation outputs\n",
    "        \"\"\"\n",
    "        # Create parameter object\n",
    "        parobj = NaghaviModelParameters()\n",
    "        \n",
    "        # Set parameters from input\n",
    "        for param_name, value in params.items():\n",
    "            if param_name == \"T\":\n",
    "                continue  # Handle cycle time separately\n",
    "            try:\n",
    "                obj, param = param_name.split('.')\n",
    "                parobj._set_comp(obj, [obj], **{param: value})\n",
    "            except Exception as e:\n",
    "                print(f\"Error setting parameter {param_name}: {e}\")\n",
    "                return None\n",
    "        \n",
    "        # Adjust cycle time if specified\n",
    "        t_cycle = params.get(\"T\", 1.0)\n",
    "        self.time_setup[\"tcycle\"] = t_cycle\n",
    "        \n",
    "        # Create and run model\n",
    "        try:\n",
    "            model = NaghaviModel(time_setup_dict=self.time_setup, parobj=parobj, suppress_printing=True)\n",
    "            solver = Solver(model=model)\n",
    "            solver.setup(suppress_output=True, optimize_secondary_sv=False, conv_cols=[\"p_ao\"], method='LSODA')\n",
    "            solver.solve()\n",
    "            \n",
    "            if not solver.converged:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Simulation error: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Process results\n",
    "        raw_results = {}\n",
    "        \n",
    "        # Get indices for the last cycle\n",
    "        tind_fin = np.arange(start=model.time_object.n_t-model.time_object.n_c,\n",
    "                            stop=model.time_object.n_t)\n",
    "        \n",
    "        # Extract raw signals of interest\n",
    "        for signal_name in self.observed_data.keys():\n",
    "            if signal_name == \"CO\":\n",
    "                # Handle cardiac output separately\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                component, _ = signal_name.split('.')\n",
    "                # Get the pressure signal from the component\n",
    "                if component in model.components:\n",
    "                    # Extract P_i (pressure) from the component for the last cycle\n",
    "                    raw_signal = model.components[component].P_i.values[tind_fin]\n",
    "                    \n",
    "                    if '_min' in signal_name:\n",
    "                        raw_results[signal_name] = np.min(raw_signal)\n",
    "                    elif '_max' in signal_name:\n",
    "                        raw_results[signal_name] = np.max(raw_signal)\n",
    "                    elif '_pc' in signal_name:\n",
    "                        # For PCA components, you would need to implement the PCA calculation\n",
    "                        # This is a placeholder\n",
    "                        pc_index = int(signal_name.split('_pc')[1])\n",
    "                        # Simple approximation for demonstration\n",
    "                        raw_results[signal_name] = np.mean(raw_signal) * (pc_index * 0.1)\n",
    "                else:\n",
    "                    print(f\"Component {component} not found in model\")\n",
    "                    return None\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting signal {signal_name}: {e}\")\n",
    "                return None\n",
    "                \n",
    "        # Add scalar metrics\n",
    "        analysis = BaseAnalysis(model)\n",
    "        analysis.compute_cardiac_output(\"lv\")\n",
    "        raw_results[\"CO\"] = analysis.CO\n",
    "        \n",
    "        return raw_results  # Return the processed results directly\n",
    "\n",
    "    def process_simulation_output(self, raw_results: Dict[str, np.ndarray]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Process raw simulation outputs to extract features for comparison with observed data\n",
    "        \n",
    "        Args:\n",
    "            raw_results: Dictionary of raw simulation outputs\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of processed features\n",
    "        \"\"\"\n",
    "        processed = {}\n",
    "        \n",
    "        # Process each signal based on what features we want to extract\n",
    "        for signal_name, signal_data in raw_results.items():\n",
    "            if signal_name == \"CO\":\n",
    "                # Cardiac output is already a scalar\n",
    "                processed[signal_name] = signal_data\n",
    "            elif '.P' in signal_name:\n",
    "                # For pressure signals, extract min, max and other features\n",
    "                processed[f\"{signal_name}_min\"] = np.min(signal_data)\n",
    "                processed[f\"{signal_name}_max\"] = np.max(signal_data)\n",
    "                \n",
    "                # Could add more sophisticated feature extraction here\n",
    "                # For example, PCA components if needed\n",
    "            \n",
    "        return processed\n",
    "    \n",
    "    def run_wave(self, samples: pd.DataFrame, n_jobs: int = 1) -> Tuple[pd.DataFrame, List[Dict], np.ndarray]:\n",
    "        \"\"\"\n",
    "        Run a wave of simulations\n",
    "        \n",
    "        Args:\n",
    "            samples: DataFrame of parameter samples\n",
    "            n_jobs: Number of parallel jobs\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (successful_samples, outputs, implausibility_scores)\n",
    "        \"\"\"\n",
    "        successful_samples = []\n",
    "        simulation_outputs = []\n",
    "        implausibility_scores = []\n",
    "\n",
    "        # Serial execution\n",
    "        for _, row in tqdm(samples.iterrows(), total=len(samples)):\n",
    "            result = self._run_single_sample(row)\n",
    "            if result is not None:\n",
    "                params, outputs, impl = result\n",
    "                successful_samples.append(params)\n",
    "                simulation_outputs.append(outputs)\n",
    "                implausibility_scores.append(impl)\n",
    "                        \n",
    "        # Create a dataframe with all outputs for inspection\n",
    "        if successful_samples:\n",
    "            # Combine parameters and outputs into one dataframe for inspection\n",
    "            output_df = pd.DataFrame(successful_samples)\n",
    "            \n",
    "            # Add implausibility scores\n",
    "            output_df['implausibility'] = implausibility_scores\n",
    "            \n",
    "            # Add simulation outputs for each observed signal\n",
    "            for i, output_dict in enumerate(simulation_outputs):\n",
    "                for signal_name, value in output_dict.items():\n",
    "                    if signal_name not in output_df.columns:\n",
    "                        output_df[signal_name] = np.nan\n",
    "                    output_df.at[i, signal_name] = value\n",
    "            \n",
    "            # Print summary statistics of the outputs\n",
    "            print(\"\\nOutput DataFrame Summary:\")\n",
    "            print(output_df.describe())\n",
    "            \n",
    "            # Show the relationship between parameters and implausibility\n",
    "            print(\"\\nCorrelation between parameters and implausibility:\")\n",
    "            for param in self.param_names:\n",
    "                corr = np.corrcoef(output_df[param], output_df['implausibility'])[0, 1]\n",
    "                print(f\"{param}: {corr:.4f}\")\n",
    "            \n",
    "            # Show the best match in this wave\n",
    "            best_idx = np.argmin(implausibility_scores)\n",
    "            print(\"\\nBest match in this wave:\")\n",
    "            print(output_df.iloc[[best_idx]])\n",
    "            \n",
    "            # Print the observed vs. simulated values for the best match\n",
    "            print(\"\\nObserved vs. Best Simulated Values:\")\n",
    "            for signal_name, observed_value in self.observed_data.items():\n",
    "                if signal_name in output_df.columns:\n",
    "                    simulated = output_df.iloc[best_idx][signal_name]\n",
    "                    print(f\"{signal_name}: Observed={observed_value:.4f}, Simulated={simulated:.4f}, Difference={abs(observed_value - simulated):.4f}\")\n",
    "            \n",
    "            # Save the output dataframe to a CSV file for external analysis\n",
    "            output_df.to_csv(f\"wave_outputs.csv\", index=False)\n",
    "            print(\"\\nOutput dataframe saved to 'wave_outputs.csv' for further analysis\")\n",
    "            \n",
    "        return pd.DataFrame(successful_samples), simulation_outputs, np.array(implausibility_scores)\n",
    "    def _run_single_sample(self, sample: pd.Series) -> Tuple[Dict, Dict, float]:\n",
    "        \"\"\"\n",
    "        Run a single sample and calculate implausibility using HistoryMatching class\n",
    "        \n",
    "        Args:\n",
    "            sample: Parameter values as Series\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (parameters, outputs, implausibility) or None if failed\n",
    "        \"\"\"\n",
    "        params = sample.to_dict()\n",
    "        outputs = self.run_simulation(params)\n",
    "        \n",
    "        if outputs is None:\n",
    "            return None\n",
    "            \n",
    "        # Calculate implausibility using the HistoryMatching class\n",
    "        implausibilities = []\n",
    "        \n",
    "        for signal_name, obs_value in self.observed_data.items():\n",
    "            if signal_name not in outputs:\n",
    "                print(f\"Warning: {signal_name} not found in simulation outputs\")\n",
    "                continue\n",
    "                \n",
    "            # Set up observation and prediction in format expected by HistoryMatching\n",
    "            # [mean, variance]\n",
    "            obs = [obs_value, 1.0]  # Assuming observation variance of 1.0\n",
    "            pred = [outputs[signal_name], 0.1]  # Assuming prediction variance of 0.1\n",
    "            \n",
    "            try:\n",
    "                result = self.history_matcher.history_matching(obs, pred)\n",
    "                implausibilities.append(result[\"I\"][0])  # Take the first implausibility value\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating implausibility for {signal_name}: {e}\")\n",
    "                implausibilities.append(float('inf'))  # Treat as infinitely implausible\n",
    "        \n",
    "        if not implausibilities:\n",
    "            return None  # No valid implausibility calculations\n",
    "            \n",
    "        impl = np.max(implausibilities)  # Maximum implausibility approach\n",
    "        \n",
    "        return params, outputs, impl\n",
    "    def history_matching(self, n_waves: int = 3, n_samples_per_wave: int = 100, n_jobs: int = 4):\n",
    "        \"\"\"\n",
    "        Perform iterative history matching using the HistoryMatching class\n",
    "        \n",
    "        Args:\n",
    "            n_waves: Number of waves to perform\n",
    "            n_samples_per_wave: Samples per wave\n",
    "            n_jobs: Number of parallel jobs\n",
    "        \"\"\"\n",
    "        current_samples = self.generate_samples(n_samples_per_wave)\n",
    "        all_samples = pd.DataFrame()\n",
    "        all_implausibilities = np.array([])\n",
    "        \n",
    "        for wave in range(n_waves):\n",
    "            print(f\"\\nStarting wave {wave + 1}/{n_waves}\")\n",
    "            \n",
    "            # Run simulations\n",
    "            successful_samples, outputs, impl_scores = self.run_wave(current_samples, n_jobs)\n",
    "            print(f\"Wave {wave+1}: {len(successful_samples)} successful samples out of {len(current_samples)}\")\n",
    "            \n",
    "            if len(successful_samples) == 0:\n",
    "                print(\"No successful simulations in this wave, generating new random samples\")\n",
    "                current_samples = self.generate_samples(n_samples_per_wave)\n",
    "                continue  # Try again with new samples instead of stopping\n",
    "                \n",
    "            # Store all results\n",
    "            all_samples = pd.concat([all_samples, successful_samples])\n",
    "            all_implausibilities = np.concatenate([all_implausibilities, impl_scores]) if len(all_implausibilities) > 0 else impl_scores\n",
    "            \n",
    "            # Identify not implausible points (NROY) using threshold from HistoryMatching\n",
    "            nroy_indices = np.where(impl_scores <= self.history_matcher.threshold)[0]\n",
    "            not_implausible = successful_samples.iloc[nroy_indices] if len(nroy_indices) > 0 else pd.DataFrame()\n",
    "            \n",
    "            print(f\"Wave {wave + 1}: {len(not_implausible)} not implausible points found (threshold: {self.history_matcher.threshold})\")\n",
    "            \n",
    "            # Prepare next wave samples\n",
    "            if wave < n_waves - 1:\n",
    "                if len(not_implausible) >= 2:  # Need at least 2 points for meaningful bounds\n",
    "                    try:\n",
    "                        print(\"Generating new samples based on not implausible points\")\n",
    "                        new_sample_points = self.history_matcher._sample_new_points(\n",
    "                            not_implausible[self.param_names].values, n_samples_per_wave)\n",
    "                        current_samples = pd.DataFrame(new_sample_points, columns=self.param_names)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error generating new samples: {e}\")\n",
    "                        print(\"Falling back to random sampling\")\n",
    "                        current_samples = self.generate_samples(n_samples_per_wave)\n",
    "                else:\n",
    "                    # Fall back to random sampling if too few NROY points\n",
    "                    print(\"Too few not implausible points, generating new random samples\")\n",
    "                    current_samples = self.generate_samples(n_samples_per_wave)\n",
    "                \n",
    "                print(f\"Generated {len(current_samples)} new samples for wave {wave+2}\")\n",
    "            \n",
    "            # Plot results only if we have implausible and not implausible points\n",
    "            if len(not_implausible) > 0 and len(successful_samples) > len(not_implausible):\n",
    "                self.plot_wave_results(wave, successful_samples, impl_scores, not_implausible)\n",
    "        \n",
    "        print(f\"\\nHistory matching completed. Total samples: {len(all_samples)}\")\n",
    "        \n",
    "        return all_samples, all_implausibilities\n",
    "    \n",
    "    def generate_samples(self, n_samples: int, method: str = 'random') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate parameter samples using specified method\n",
    "        \n",
    "        Args:\n",
    "            n_samples: Number of samples to generate\n",
    "            method: Sampling method ('random' or 'lhs')\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame of parameter samples\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        \n",
    "        if method == 'random':\n",
    "            for param_name, (low, high) in self.param_ranges.items():\n",
    "                samples.append(np.random.uniform(low, high, n_samples))\n",
    "        elif method == 'lhs':\n",
    "            # Implement Latin Hypercube Sampling here if needed\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sampling method: {method}\")\n",
    "            \n",
    "        return pd.DataFrame(np.array(samples).T, columns=self.param_names)\n",
    "    \n",
    "    def plot_wave_results(self, wave: int, samples: pd.DataFrame, \n",
    "                        impl_scores: np.ndarray, not_implausible: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Plot results from a history matching wave\n",
    "        \n",
    "        Args:\n",
    "            wave: Wave number\n",
    "            samples: All successful samples\n",
    "            impl_scores: Corresponding implausibility scores\n",
    "            not_implausible: Not implausible samples\n",
    "        \"\"\"\n",
    "        if len(self.param_names) < 2:\n",
    "            return\n",
    "            \n",
    "        # Select two main parameters to plot\n",
    "        param1, param2 = self.param_names[:2]\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Plot all samples colored by implausibility\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sc = plt.scatter(samples[param1], samples[param2], c=impl_scores, \n",
    "                        cmap='viridis_r', vmax=3.0)\n",
    "        plt.colorbar(sc, label='Implausibility')\n",
    "        plt.xlabel(param1)\n",
    "        plt.ylabel(param2)\n",
    "        plt.title(f'Wave {wave + 1} - All Samples')\n",
    "        \n",
    "        # Plot not implausible region\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(not_implausible[param1], not_implausible[param2], \n",
    "                c='blue', alpha=0.5)\n",
    "        plt.xlabel(param1)\n",
    "        plt.ylabel(param2)\n",
    "        plt.title(f'Wave {wave + 1} - Not Implausible Region')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def calculate_implausibility(self, sim_output: Dict[str, float], \n",
    "                            obs_error: Dict[str, float], \n",
    "                            model_error: Dict[str, float],\n",
    "                            emulator_error: Dict[str, float] = None) -> float:\n",
    "        \"\"\"\n",
    "        Calculate combined implausibility metric across all outputs\n",
    "        \n",
    "        Args:\n",
    "            sim_output: Simulator outputs (max values)\n",
    "            obs_error: Observation errors for each output\n",
    "            model_error: Model errors for each output\n",
    "            emulator_error: Optional emulator errors\n",
    "            \n",
    "        Returns:\n",
    "            Combined implausibility score\n",
    "        \"\"\"\n",
    "        total_implausibility = 0.0\n",
    "        \n",
    "        for signal_name, obs_value in self.observed_data.items():\n",
    "            sim_value = sim_output[signal_name]\n",
    "            error = obs_error.get(signal_name, 0.1)\n",
    "            m_error = model_error.get(signal_name, 0.05)\n",
    "            \n",
    "            # Add emulator error if provided\n",
    "            e_error = emulator_error.get(signal_name, 0.0) if emulator_error else 0.0\n",
    "            \n",
    "            # Calculate absolute difference\n",
    "            diff = np.abs(sim_value - obs_value)\n",
    "            impl = diff / np.sqrt(error**2 + m_error**2 + e_error**2)\n",
    "            total_implausibility += impl\n",
    "                \n",
    "        return total_implausibility\n",
    "\n",
    "    def calculate_implausibility(self, sim_output: Dict[str, float], \n",
    "                                obs_error: Dict[str, float], \n",
    "                                model_error: Dict[str, float],\n",
    "                                emulator_error: Dict[str, float] = None) -> float:\n",
    "        \"\"\"\n",
    "        Calculate combined implausibility metric across all processed outputs\n",
    "        \n",
    "        Args:\n",
    "            sim_output: Simulator processed outputs\n",
    "            obs_error: Observation errors for each output\n",
    "            model_error: Model errors for each output\n",
    "            emulator_error: Optional emulator errors\n",
    "            \n",
    "        Returns:\n",
    "            Combined implausibility score\n",
    "        \"\"\"\n",
    "        implausibilities = []\n",
    "        \n",
    "        for signal_name, obs_value in self.observed_data.items():\n",
    "            if signal_name not in sim_output:\n",
    "                print(f\"Warning: {signal_name} not in simulation output\")\n",
    "                continue\n",
    "                \n",
    "            sim_value = sim_output[signal_name]\n",
    "            error = obs_error.get(signal_name, 0.1)\n",
    "            m_error = model_error.get(signal_name, 0.05)\n",
    "            \n",
    "            # Add emulator error if provided\n",
    "            e_error = emulator_error.get(signal_name, 0.0) if emulator_error else 0.0\n",
    "            \n",
    "            # Calculate absolute difference\n",
    "            diff = np.abs(sim_value - obs_value)\n",
    "            impl = diff / np.sqrt(error**2 + m_error**2 + e_error**2)\n",
    "            implausibilities.append(impl)\n",
    "                \n",
    "        # Return maximum implausibility (conservative approach)\n",
    "        # Alternative: return np.mean(implausibilities) for average implausibility\n",
    "        return np.max(implausibilities)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define observed data (would normally load from file)\n",
    "\n",
    "    observed_signals = {\n",
    "        \"ao.P_min\": 80.0,    # Minimum observed aortic pressure (mmHg)\n",
    "        \"ao.P_max\": 120.0,   # Maximum observed aortic pressure (mmHg)\n",
    "        \"ao.P_pc1\": 10.5,    # First PCA component of aortic pressure\n",
    "        \"ao.P_pc2\": -3.2,    # Second PCA component of aortic pressure\n",
    "        \"ao.P_pc3\": 0.8,     # Third PCA component of aortic pressure\n",
    "        \"CO\": 5.0            # Observed cardiac output in L/min\n",
    "    }\n",
    "\n",
    "\n",
    "# Updated parameter ranges\n",
    "    param_ranges = {\n",
    "        \"lv.E_act\": (0.0, 20.0),  # LV active elastance (similar to end-systolic elastance)\n",
    "        \"lv.v_ref\": (0.0, 20.0),  # LV reference volume (similar to dead volume)\n",
    "        \"la.E_act\": (0., 10.5),   # LA active elastance\n",
    "        # For systemic arterial resistance, we need to check what parameter name to use\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Create history matcher instance\n",
    "    hm = NaghaviHistoryMatcher(observed_data=observed_signals, param_ranges=param_ranges)\n",
    "    \n",
    "    # Run history matching\n",
    "    best_samples, best_impl_scores = hm.history_matching(n_waves=5, n_samples_per_wave=50)\n",
    "    \n",
    "    # Print best match\n",
    "    best_idx = np.argmin(best_impl_scores)\n",
    "    print(\"\\nBest matching parameters:\")\n",
    "    print(best_samples.iloc[best_idx])\n",
    "    print(f\"Implausibility: {best_impl_scores[best_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HistoryMatching' from 'autoemulate.history_matching' (/Users/mfamili/work/autoemulate/autoemulate/history_matching.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Tuple, List\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mautoemulate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhistory_matching\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HistoryMatching\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mNaghaviHistoryMatcher\u001b[39;00m:\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, observed_data: Dict[\u001b[38;5;28mstr\u001b[39m, np.ndarray], \n\u001b[32m     19\u001b[39m                  param_ranges: Dict[\u001b[38;5;28mstr\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]],\n\u001b[32m     20\u001b[39m                  n_cycles: \u001b[38;5;28mint\u001b[39m = \u001b[32m40\u001b[39m, dt: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.001\u001b[39m,\n\u001b[32m     21\u001b[39m                  implausibility_threshold: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m3.0\u001b[39m,\n\u001b[32m     22\u001b[39m                  model_discrepancy: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.0\u001b[39m):\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'HistoryMatching' from 'autoemulate.history_matching' (/Users/mfamili/work/autoemulate/autoemulate/history_matching.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import uniform\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from ModularCirc.Models.NaghaviModel import NaghaviModel, NaghaviModelParameters\n",
    "from ModularCirc.Analysis.BaseAnalysis import BaseAnalysis\n",
    "from ModularCirc.Solver import Solver\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Tuple, List\n",
    "from autoemulate.history_matching import HistoryMatching\n",
    "\n",
    "\n",
    "class NaghaviHistoryMatcher:\n",
    "    def __init__(self, observed_data: Dict[str, np.ndarray], \n",
    "                 param_ranges: Dict[str, Tuple[float, float]],\n",
    "                 n_cycles: int = 40, dt: float = 0.001,\n",
    "                 implausibility_threshold: float = 3.0,\n",
    "                 model_discrepancy: float = 0.0):\n",
    "        \"\"\"\n",
    "        Initialize the history matching framework for Naghavi model\n",
    "        \n",
    "        Args:\n",
    "            observed_data: Dictionary containing observed signals (keys: signal names, values: arrays)\n",
    "            param_ranges: Dictionary of parameter ranges to explore (keys: parameter names, values: (min, max))\n",
    "            n_cycles: Number of cardiac cycles to simulate\n",
    "            dt: Time step for simulation\n",
    "            implausibility_threshold: Threshold for considering points implausible\n",
    "            model_discrepancy: Model discrepancy term\n",
    "        \"\"\"\n",
    "        self.observed_data = observed_data\n",
    "        self.param_ranges = param_ranges\n",
    "        self.param_names = list(param_ranges.keys())\n",
    "        self.n_cycles = n_cycles\n",
    "        self.dt = dt\n",
    "        self.samples = None\n",
    "        self.simulator_outputs = None\n",
    "        self.gp_emulators = {}\n",
    "        \n",
    "        # Initialize the HistoryMatching object\n",
    "        self.history_matcher = HistoryMatching(\n",
    "            threshold=implausibility_threshold,\n",
    "            discrepancy=model_discrepancy,\n",
    "            rank=1\n",
    "        )\n",
    "        \n",
    "        # Time setup dictionary\n",
    "        self.time_setup = {\n",
    "            \"name\": \"HistoryMatching\",\n",
    "            \"ncycles\": n_cycles,\n",
    "            \"tcycle\": 1.0,  # Will be adjusted per simulation\n",
    "            \"dt\": dt,\n",
    "            \"export_min\": 1\n",
    "        }\n",
    "        # Initialize emulator\n",
    "        self.emulator = None\n",
    "        self.emulator_trained = False\n",
    "\n",
    "    def train_emulator(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"Train the GP emulator using autoemulate\"\"\"\n",
    "        from autoemulate.compare import AutoEmulate\n",
    "        \n",
    "        print(\"Training GP emulator...\")\n",
    "        em = AutoEmulate()\n",
    "        em.setup(X, y, models=[\"gp\"])\n",
    "        self.emulator = em.compare()\n",
    "        self.emulator_trained = True\n",
    "        print(\"GP emulator trained successfully\")\n",
    "        \n",
    "    def get_emulator_predictions(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Get predictions from trained emulator\"\"\"\n",
    "        if not self.emulator_trained:\n",
    "            raise ValueError(\"Emulator not trained yet\")\n",
    "            \n",
    "        pred_mean, pred_std = self.emulator.predict(X, return_std=True)\n",
    "        pred_var = np.square(pred_std)\n",
    "        return pred_mean, pred_var\n",
    "\n",
    "    def signal_get_pulse(self, signal: np.ndarray, dt: float, num: int = 100) -> Tuple[float, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Resample raw signal to standard resolution\n",
    "        \n",
    "        Args:\n",
    "            signal: Raw signal array\n",
    "            dt: Original time resolution\n",
    "            num: Number of points in resampled signal\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (new_dt, resampled_signal)\n",
    "        \"\"\"\n",
    "        ind = np.argmin(signal)        \n",
    "        ncycle = len(signal)\n",
    "        new_signal = np.interp(np.linspace(0, ncycle, num), \n",
    "                              np.arange(ncycle), \n",
    "                              np.roll(signal, -ind))\n",
    "        new_dt = ncycle / (num - 1) * dt\n",
    "        return new_dt, new_signal\n",
    "    \n",
    "    def run_simulation(self, params: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Run a single Naghavi model simulation with given parameters\n",
    "        \n",
    "        Args:\n",
    "            params: Dictionary of parameter values\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of processed features from simulation outputs\n",
    "        \"\"\"\n",
    "        # Create parameter object\n",
    "        parobj = NaghaviModelParameters()\n",
    "        \n",
    "        # Set parameters from input\n",
    "        for param_name, value in params.items():\n",
    "            if param_name == \"T\":\n",
    "                continue  # Handle cycle time separately\n",
    "            try:\n",
    "                obj, param = param_name.split('.')\n",
    "                parobj._set_comp(obj, [obj], **{param: value})\n",
    "            except Exception as e:\n",
    "                print(f\"Error setting parameter {param_name}: {e}\")\n",
    "                return None\n",
    "        \n",
    "        # Adjust cycle time if specified\n",
    "        t_cycle = params.get(\"T\", 1.0)\n",
    "        self.time_setup[\"tcycle\"] = t_cycle\n",
    "        \n",
    "        # Create and run model\n",
    "        try:\n",
    "            model = NaghaviModel(time_setup_dict=self.time_setup, parobj=parobj, suppress_printing=True)\n",
    "            solver = Solver(model=model)\n",
    "            solver.setup(suppress_output=True, optimize_secondary_sv=False, conv_cols=[\"p_ao\"], method='LSODA')\n",
    "            solver.solve()\n",
    "            \n",
    "            if not solver.converged:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Simulation error: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Process results\n",
    "        raw_results = {}\n",
    "        \n",
    "        # Get indices for the last cycle\n",
    "        tind_fin = np.arange(start=model.time_object.n_t-model.time_object.n_c,\n",
    "                            stop=model.time_object.n_t)\n",
    "        \n",
    "        # Extract raw signals of interest - only min and max values\n",
    "        for signal_name in self.observed_data.keys():\n",
    "            try:\n",
    "                component, _ = signal_name.split('.')\n",
    "                # Get the pressure signal from the component\n",
    "                if component in model.components:\n",
    "                    # Extract P_i (pressure) from the component for the last cycle\n",
    "                    raw_signal = model.components[component].P_i.values[tind_fin]\n",
    "                    \n",
    "                    if '_min' in signal_name:\n",
    "                        raw_results[signal_name] = np.min(raw_signal)\n",
    "                    elif '_max' in signal_name:\n",
    "                        raw_results[signal_name] = np.max(raw_signal)\n",
    "                else:\n",
    "                    print(f\"Component {component} not found in model\")\n",
    "                    return None\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting signal {signal_name}: {e}\")\n",
    "                return None\n",
    "                \n",
    "        return raw_results  # Return the processed results directly\n",
    "\n",
    "    def process_simulation_output(self, raw_results: Dict[str, np.ndarray]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Process raw simulation outputs to extract features for comparison with observed data\n",
    "        \n",
    "        Args:\n",
    "            raw_results: Dictionary of raw simulation outputs\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of processed features\n",
    "        \"\"\"\n",
    "        processed = {}\n",
    "        \n",
    "        # Process each signal based on what features we want to extract\n",
    "        for signal_name, signal_data in raw_results.items():\n",
    "            if '.P' in signal_name:\n",
    "                # For pressure signals, extract min and max\n",
    "                if '_min' in signal_name:\n",
    "                    processed[f\"{signal_name}\"] = np.min(signal_data)\n",
    "                elif '_max' in signal_name:\n",
    "                    processed[f\"{signal_name}\"] = np.max(signal_data)\n",
    "            \n",
    "        return processed\n",
    "    \n",
    "    def run_wave(self, samples: pd.DataFrame, n_jobs: int = 1) -> Tuple[pd.DataFrame, List[Dict], np.ndarray]:\n",
    "        \"\"\"\n",
    "        Run a wave of simulations\n",
    "        \n",
    "        Args:\n",
    "            samples: DataFrame of parameter samples\n",
    "            n_jobs: Number of parallel jobs\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (successful_samples, outputs, implausibility_scores)\n",
    "        \"\"\"\n",
    "        successful_samples = []\n",
    "        simulation_outputs = []\n",
    "        implausibility_scores = []\n",
    "\n",
    "        # Serial execution\n",
    "        for _, row in tqdm(samples.iterrows(), total=len(samples)):\n",
    "            result = self._run_single_sample(row)\n",
    "            if result is not None:\n",
    "                params, outputs, impl = result\n",
    "                successful_samples.append(params)\n",
    "                simulation_outputs.append(outputs)\n",
    "                implausibility_scores.append(impl)\n",
    "                        \n",
    "        # Create a dataframe with all outputs for inspection\n",
    "        if successful_samples:\n",
    "            # Combine parameters and outputs into one dataframe for inspection\n",
    "            output_df = pd.DataFrame(successful_samples)\n",
    "            \n",
    "            # Add implausibility scores\n",
    "            output_df['implausibility'] = implausibility_scores\n",
    "            \n",
    "            # Add simulation outputs for each observed signal\n",
    "            for i, output_dict in enumerate(simulation_outputs):\n",
    "                for signal_name, value in output_dict.items():\n",
    "                    if signal_name not in output_df.columns:\n",
    "                        output_df[signal_name] = np.nan\n",
    "                    output_df.at[i, signal_name] = value\n",
    "            \n",
    "            # Print summary statistics of the outputs\n",
    "            print(\"\\nOutput DataFrame Summary:\")\n",
    "            print(output_df.describe())\n",
    "            \n",
    "            # Show the relationship between parameters and implausibility\n",
    "            print(\"\\nCorrelation between parameters and implausibility:\")\n",
    "            for param in self.param_names:\n",
    "                corr = np.corrcoef(output_df[param], output_df['implausibility'])[0, 1]\n",
    "                print(f\"{param}: {corr:.4f}\")\n",
    "            \n",
    "            # Show the best match in this wave\n",
    "            best_idx = np.argmin(implausibility_scores)\n",
    "            print(\"\\nBest match in this wave:\")\n",
    "            print(output_df.iloc[[best_idx]])\n",
    "            \n",
    "            # Print the observed vs. simulated values for the best match\n",
    "            print(\"\\nObserved vs. Best Simulated Values:\")\n",
    "            for signal_name, observed_value in self.observed_data.items():\n",
    "                if signal_name in output_df.columns:\n",
    "                    simulated = output_df.iloc[best_idx][signal_name]\n",
    "                    print(f\"{signal_name}: Observed={observed_value:.4f}, Simulated={simulated:.4f}, Difference={abs(observed_value - simulated):.4f}\")\n",
    "            \n",
    "            # Save the output dataframe to a CSV file for external analysis\n",
    "            output_df.to_csv(f\"wave_outputs.csv\", index=False)\n",
    "            print(\"\\nOutput dataframe saved to 'wave_outputs.csv' for further analysis\")\n",
    "            \n",
    "        return pd.DataFrame(successful_samples), simulation_outputs, np.array(implausibility_scores)\n",
    "    \n",
    "    \n",
    "    def _run_single_sample(self, sample: pd.Series) -> Tuple[Dict, Dict, float]:\n",
    "        \"\"\"\n",
    "        Run a single sample and calculate implausibility using emulator if available\n",
    "        \"\"\"\n",
    "        params = sample.to_dict()\n",
    "        outputs = self.run_simulation(params)\n",
    "        \n",
    "        if outputs is None:\n",
    "            return None\n",
    "            \n",
    "        # Prepare predictions and observations for history matching\n",
    "        predictions = []\n",
    "        observations = []\n",
    "        \n",
    "        for signal_name, obs_value in self.observed_data.items():\n",
    "            if signal_name not in outputs:\n",
    "                print(f\"Warning: {signal_name} not found in simulation outputs\")\n",
    "                continue\n",
    "                \n",
    "            # For direct simulation (no emulator)\n",
    "            pred_mean = outputs[signal_name]\n",
    "            pred_var = 0.01  # Small fixed variance for direct simulation\n",
    "            \n",
    "            # For emulator, we would use:\n",
    "            # pred_mean, pred_var = self.get_emulator_predictions(...)\n",
    "            \n",
    "            predictions.append([pred_mean, pred_var])\n",
    "            observations.append([obs_value, 0.1])  # obs_value and obs_variance\n",
    "            \n",
    "        if not predictions:\n",
    "            return None\n",
    "            \n",
    "        # Calculate implausibility using HistoryMatching\n",
    "        try:\n",
    "            result = self.history_matcher.history_matching(observations, predictions)\n",
    "            impl = result[\"I\"][0]  # Take the first implausibility value\n",
    "            return params, outputs, impl\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating implausibility: {e}\")\n",
    "            return None\n",
    "            \n",
    "    def history_matching(self, n_waves: int = 3, n_samples_per_wave: int = 100, n_jobs: int = 4):\n",
    "        \"\"\"\n",
    "        Perform iterative history matching using the HistoryMatching class\n",
    "        \n",
    "        Args:\n",
    "            n_waves: Number of waves to perform\n",
    "            n_samples_per_wave: Samples per wave\n",
    "            n_jobs: Number of parallel jobs\n",
    "        \"\"\"\n",
    "        current_samples = self.generate_samples(n_samples_per_wave)\n",
    "        all_samples = pd.DataFrame()\n",
    "        all_implausibilities = np.array([])\n",
    "        \n",
    "        for wave in range(n_waves):\n",
    "            print(f\"\\nStarting wave {wave + 1}/{n_waves}\")\n",
    "            \n",
    "            # Run simulations\n",
    "            successful_samples, outputs, impl_scores = self.run_wave(current_samples, n_jobs)\n",
    "            print(f\"Wave {wave+1}: {len(successful_samples)} successful samples out of {len(current_samples)}\")\n",
    "            \n",
    "            if len(successful_samples) == 0:\n",
    "                print(\"No successful simulations in this wave, generating new random samples\")\n",
    "                current_samples = self.generate_samples(n_samples_per_wave)\n",
    "                continue  # Try again with new samples instead of stopping\n",
    "                \n",
    "            # Store all results\n",
    "            all_samples = pd.concat([all_samples, successful_samples])\n",
    "            all_implausibilities = np.concatenate([all_implausibilities, impl_scores]) if len(all_implausibilities) > 0 else impl_scores\n",
    "            \n",
    "            # Identify not implausible points (NROY) using threshold from HistoryMatching\n",
    "            nroy_indices = np.where(impl_scores <= self.history_matcher.threshold)[0]\n",
    "            not_implausible = successful_samples.iloc[nroy_indices] if len(nroy_indices) > 0 else pd.DataFrame()\n",
    "            \n",
    "            print(f\"Wave {wave + 1}: {len(not_implausible)} not implausible points found (threshold: {self.history_matcher.threshold})\")\n",
    "            \n",
    "            # Prepare next wave samples\n",
    "            if wave < n_waves - 1:\n",
    "                if len(not_implausible) >= 2:  # Need at least 2 points for meaningful bounds\n",
    "                    try:\n",
    "                        print(\"Generating new samples based on not implausible points\")\n",
    "                        new_sample_points = self.history_matcher._sample_new_points(\n",
    "                            not_implausible[self.param_names].values, n_samples_per_wave)\n",
    "                        current_samples = pd.DataFrame(new_sample_points, columns=self.param_names)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error generating new samples: {e}\")\n",
    "                        print(\"Falling back to random sampling\")\n",
    "                        current_samples = self.generate_samples(n_samples_per_wave)\n",
    "                else:\n",
    "                    # Fall back to random sampling if too few NROY points\n",
    "                    print(\"Too few not implausible points, generating new random samples\")\n",
    "                    current_samples = self.generate_samples(n_samples_per_wave)\n",
    "                \n",
    "                print(f\"Generated {len(current_samples)} new samples for wave {wave+2}\")\n",
    "            \n",
    "            # Plot results only if we have implausible and not implausible points\n",
    "            if len(not_implausible) > 0 and len(successful_samples) > len(not_implausible):\n",
    "                self.plot_wave_results(wave, successful_samples, impl_scores, not_implausible)\n",
    "        \n",
    "        print(f\"\\nHistory matching completed. Total samples: {len(all_samples)}\")\n",
    "        \n",
    "        return all_samples, all_implausibilities\n",
    "    \n",
    "    def generate_samples(self, n_samples: int, method: str = 'random') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate parameter samples using specified method\n",
    "        \n",
    "        Args:\n",
    "            n_samples: Number of samples to generate\n",
    "            method: Sampling method ('random' or 'lhs')\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame of parameter samples\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        \n",
    "        if method == 'random':\n",
    "            for param_name, (low, high) in self.param_ranges.items():\n",
    "                samples.append(np.random.uniform(low, high, n_samples))\n",
    "        elif method == 'lhs':\n",
    "            # Implement Latin Hypercube Sampling here if needed\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sampling method: {method}\")\n",
    "            \n",
    "        return pd.DataFrame(np.array(samples).T, columns=self.param_names)\n",
    "    \n",
    "    def plot_wave_results(self, wave: int, samples: pd.DataFrame, \n",
    "                         impl_scores: np.ndarray, not_implausible: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Plot results from a history matching wave\n",
    "        \n",
    "        Args:\n",
    "            wave: Wave number\n",
    "            samples: All successful samples\n",
    "            impl_scores: Corresponding implausibility scores\n",
    "            not_implausible: Not implausible samples\n",
    "        \"\"\"\n",
    "        if len(self.param_names) < 2:\n",
    "            return\n",
    "            \n",
    "        # Select two main parameters to plot\n",
    "        param1, param2 = self.param_names[:2]\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Plot all samples colored by implausibility\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sc = plt.scatter(samples[param1], samples[param2], c=impl_scores, \n",
    "                        cmap='viridis_r', vmax=3.0)\n",
    "        plt.colorbar(sc, label='Implausibility')\n",
    "        plt.xlabel(param1)\n",
    "        plt.ylabel(param2)\n",
    "        plt.title(f'Wave {wave + 1} - All Samples')\n",
    "        \n",
    "        # Plot not implausible region\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(not_implausible[param1], not_implausible[param2], \n",
    "                  c='blue', alpha=0.5)\n",
    "        plt.xlabel(param1)\n",
    "        plt.ylabel(param2)\n",
    "        plt.title(f'Wave {wave + 1} - Not Implausible Region')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def calculate_implausibility(self, sim_output: Dict[str, float], \n",
    "                            obs_error: Dict[str, float], \n",
    "                            model_error: Dict[str, float],\n",
    "                            emulator_error: Dict[str, float] = None) -> float:\n",
    "        \"\"\"\n",
    "        Calculate combined implausibility metric across all outputs\n",
    "        \n",
    "        Args:\n",
    "            sim_output: Simulator outputs (max values)\n",
    "            obs_error: Observation errors for each output\n",
    "            model_error: Model errors for each output\n",
    "            emulator_error: Optional emulator errors\n",
    "            \n",
    "        Returns:\n",
    "            Combined implausibility score\n",
    "        \"\"\"\n",
    "        total_implausibility = 0.0\n",
    "        \n",
    "        for signal_name, obs_value in self.observed_data.items():\n",
    "            sim_value = sim_output[signal_name]\n",
    "            error = obs_error.get(signal_name, 0.1)\n",
    "            m_error = model_error.get(signal_name, 0.05)\n",
    "            \n",
    "            # Add emulator error if provided\n",
    "            e_error = emulator_error.get(signal_name, 0.0) if emulator_error else 0.0\n",
    "            \n",
    "            # Calculate absolute difference\n",
    "            diff = np.abs(sim_value - obs_value)\n",
    "            impl = diff / np.sqrt(error**2 + m_error**2 + e_error**2)\n",
    "            total_implausibility += impl\n",
    "                \n",
    "        return total_implausibility\n",
    "\n",
    "    def calculate_implausibility(self, sim_output: Dict[str, float], \n",
    "                                obs_error: Dict[str, float], \n",
    "                                model_error: Dict[str, float],\n",
    "                                emulator_error: Dict[str, float] = None) -> float:\n",
    "        \"\"\"\n",
    "        Calculate combined implausibility metric across all processed outputs\n",
    "        \n",
    "        Args:\n",
    "            sim_output: Simulator processed outputs\n",
    "            obs_error: Observation errors for each output\n",
    "            model_error: Model errors for each output\n",
    "            emulator_error: Optional emulator errors\n",
    "            \n",
    "        Returns:\n",
    "            Combined implausibility score\n",
    "        \"\"\"\n",
    "        implausibilities = []\n",
    "        \n",
    "        for signal_name, obs_value in self.observed_data.items():\n",
    "            if signal_name not in sim_output:\n",
    "                print(f\"Warning: {signal_name} not in simulation output\")\n",
    "                continue\n",
    "                \n",
    "            sim_value = sim_output[signal_name]\n",
    "            error = obs_error.get(signal_name, 0.1)\n",
    "            m_error = model_error.get(signal_name, 0.05)\n",
    "            \n",
    "            # Add emulator error if provided\n",
    "            e_error = emulator_error.get(signal_name, 0.0) if emulator_error else 0.0\n",
    "            \n",
    "            # Calculate absolute difference\n",
    "            diff = np.abs(sim_value - obs_value)\n",
    "            impl = diff / np.sqrt(error**2 + m_error**2 + e_error**2)\n",
    "            implausibilities.append(impl)\n",
    "                \n",
    "        # Return maximum implausibility (conservative approach)\n",
    "        # Alternative: return np.mean(implausibilities) for average implausibility\n",
    "        return np.max(implausibilities)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define observed data (would normally load from file)\n",
    "    observed_signals = {\n",
    "        \"ao.P_min\": 80.0,    # Minimum observed aortic pressure (mmHg)\n",
    "        \"ao.P_max\": 120.0,   # Maximum observed aortic pressure (mmHg)\n",
    "    }\n",
    "\n",
    "    # Updated parameter ranges\n",
    "    param_ranges = {\n",
    "        \"lv.E_act\": (0.0, 20.0),  # LV active elastance (similar to end-systolic elastance)\n",
    "        \"lv.v_ref\": (0.0, 20.0),  # LV reference volume (similar to dead volume)\n",
    "        \"la.E_act\": (0., 10.5),   # LA active elastance\n",
    "        # For systemic arterial resistance, we need to check what parameter name to use\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Create history matcher instance\n",
    "    hm = NaghaviHistoryMatcher(observed_data=observed_signals, param_ranges=param_ranges)\n",
    "    \n",
    "    # Run history matching\n",
    "    best_samples, best_impl_scores = hm.history_matching(n_waves=5, n_samples_per_wave=50)\n",
    "    \n",
    "    # Print best match\n",
    "    best_idx = np.argmin(best_impl_scores)\n",
    "    print(\"\\nBest matching parameters:\")\n",
    "    print(best_samples.iloc[best_idx])\n",
    "    print(f\"Implausibility: {best_impl_scores[best_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HistoryMatching' from 'autoemulate.history_matching' (/Users/mfamili/work/autoemulate/autoemulate/history_matching.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Tuple, List\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mautoemulate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompare\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoEmulate\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mautoemulate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhistory_matching\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HistoryMatching\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mModularCirc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mModels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mNaghaviModel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NaghaviModel, NaghaviModelParameters\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/autoemulate/autoemulate/compare.py:31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mautoemulate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_model_name\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mautoemulate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_short_model_name\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mautoemulate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhistory_matching\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HistoryMatching\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mAutoEmulate\u001b[39;00m:\n\u001b[32m     34\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[33;03m    The AutoEmulate class is the main class of the AutoEmulate package. It is used to set up and compare\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03m    different emulator models on a given dataset. It can also be used to summarise and visualise results,\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03m    to save and load models and to run sensitivity analysis.\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'HistoryMatching' from 'autoemulate.history_matching' (/Users/mfamili/work/autoemulate/autoemulate/history_matching.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Tuple, List\n",
    "from autoemulate.compare import AutoEmulate\n",
    "from autoemulate.history_matching import HistoryMatching\n",
    "from ModularCirc.Models.NaghaviModel import NaghaviModel, NaghaviModelParameters\n",
    "from ModularCirc.Solver import Solver\n",
    "\n",
    "class NaghaviHistoryMatcher:\n",
    "    def __init__(self, observed_data: Dict[str, float], \n",
    "                 param_ranges: Dict[str, Tuple[float, float]],\n",
    "                 n_cycles: int = 40, dt: float = 0.001,\n",
    "                 implausibility_threshold: float = 3.0,\n",
    "                 model_discrepancy: float = 0.0):\n",
    "        \"\"\"\n",
    "        Initialize the history matching framework for Naghavi model\n",
    "        \n",
    "        Args:\n",
    "            observed_data: Dictionary of target values {signal_name: target_value}\n",
    "            param_ranges: Dictionary of parameter ranges {param_name: (min, max)}\n",
    "            n_cycles: Number of cardiac cycles to simulate\n",
    "            dt: Time step for simulation\n",
    "            implausibility_threshold: Threshold for implausibility\n",
    "            model_discrepancy: Model discrepancy term\n",
    "        \"\"\"\n",
    "        self.observed_data = observed_data\n",
    "        self.param_ranges = param_ranges\n",
    "        self.param_names = list(param_ranges.keys())\n",
    "        self.n_cycles = n_cycles\n",
    "        self.dt = dt\n",
    "        \n",
    "        # Time setup dictionary\n",
    "        self.time_setup = {\n",
    "            \"name\": \"HistoryMatching\",\n",
    "            \"ncycles\": n_cycles,\n",
    "            \"tcycle\": 1.0,\n",
    "            \"dt\": dt,\n",
    "            \"export_min\": 1\n",
    "        }\n",
    "        \n",
    "        # Initialize emulator\n",
    "        self.emulator = None\n",
    "        self.emulator_trained = False\n",
    "        \n",
    "        # History matching setup\n",
    "        self.history_matcher = HistoryMatching(\n",
    "            threshold=implausibility_threshold,\n",
    "            discrepancy=model_discrepancy,\n",
    "            rank=1\n",
    "        )\n",
    "\n",
    "    def run_simulation(self, params: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Run a single Naghavi model simulation with given parameters\n",
    "        Returns dictionary of output values {signal_name: value}\n",
    "        \"\"\"\n",
    "        # Create parameter object\n",
    "        parobj = NaghaviModelParameters()\n",
    "        \n",
    "        # Set parameters from input\n",
    "        for param_name, value in params.items():\n",
    "            if param_name == \"T\":\n",
    "                continue  # Handle cycle time separately\n",
    "            try:\n",
    "                obj, param = param_name.split('.')\n",
    "                parobj._set_comp(obj, [obj], **{param: value})\n",
    "            except Exception as e:\n",
    "                print(f\"Error setting parameter {param_name}: {e}\")\n",
    "                return None\n",
    "        \n",
    "        # Adjust cycle time if specified\n",
    "        t_cycle = params.get(\"T\", 1.0)\n",
    "        self.time_setup[\"tcycle\"] = t_cycle\n",
    "        \n",
    "        # Create and run model\n",
    "        try:\n",
    "            model = NaghaviModel(time_setup_dict=self.time_setup, parobj=parobj, suppress_printing=True)\n",
    "            solver = Solver(model=model)\n",
    "            solver.setup(suppress_output=True, optimize_secondary_sv=False, conv_cols=[\"p_ao\"], method='LSODA')\n",
    "            solver.solve()\n",
    "            \n",
    "            if not solver.converged:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Simulation error: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Process results - extract last cycle min/max values\n",
    "        raw_results = {}\n",
    "        tind_fin = np.arange(start=model.time_object.n_t-model.time_object.n_c,\n",
    "                            stop=model.time_object.n_t)\n",
    "        \n",
    "        for signal_name in self.observed_data.keys():\n",
    "            try:\n",
    "                component, _ = signal_name.split('.')\n",
    "                if component in model.components:\n",
    "                    raw_signal = model.components[component].P_i.values[tind_fin]\n",
    "                    \n",
    "                    if '_min' in signal_name:\n",
    "                        raw_results[signal_name] = np.min(raw_signal)\n",
    "                    elif '_max' in signal_name:\n",
    "                        raw_results[signal_name] = np.max(raw_signal)\n",
    "                else:\n",
    "                    print(f\"Component {component} not found in model\")\n",
    "                    return None\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting signal {signal_name}: {e}\")\n",
    "                return None\n",
    "                \n",
    "        return raw_results\n",
    "\n",
    "    def train_emulator(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"Train the GP emulator using autoemulate\"\"\"\n",
    "        print(\"Training GP emulator...\")\n",
    "        em = AutoEmulate()\n",
    "        em.setup(X, y, models=[\"gp\"])\n",
    "        self.emulator = em.compare()\n",
    "        self.emulator_trained = True\n",
    "        print(f\"GP emulator trained on {len(X)} samples\")\n",
    "\n",
    "    def get_emulator_predictions(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Get predictions from trained emulator\n",
    "        Returns:\n",
    "            pred_mean: Array of shape (n_samples, n_outputs)\n",
    "            pred_var: Array of shape (n_samples, n_outputs)\n",
    "        \"\"\"\n",
    "        if not self.emulator_trained:\n",
    "            raise ValueError(\"Emulator not trained yet\")\n",
    "            \n",
    "        pred_mean, pred_std = self.emulator.predict(X, return_std=True)\n",
    "        pred_var = np.square(pred_std)\n",
    "        return pred_mean, pred_var\n",
    "\n",
    "    def generate_samples(self, n_samples: int) -> pd.DataFrame:\n",
    "        \"\"\"Generate random parameter samples\"\"\"\n",
    "        samples = []\n",
    "        for param_name, (low, high) in self.param_ranges.items():\n",
    "            samples.append(np.random.uniform(low, high, n_samples))\n",
    "        return pd.DataFrame(np.array(samples).T, columns=self.param_names)\n",
    "\n",
    "    def run_wave(self, samples: pd.DataFrame, use_emulator: bool = False) -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Run a wave of simulations or emulator predictions\n",
    "        Returns:\n",
    "            successful_samples: DataFrame of parameter sets\n",
    "            impl_scores: Array of implausibility scores\n",
    "        \"\"\"\n",
    "        if use_emulator and self.emulator_trained:\n",
    "            # Use emulator for predictions\n",
    "            X = samples[self.param_names].values\n",
    "            pred_mean, pred_var = self.get_emulator_predictions(X)\n",
    "            \n",
    "            # Calculate implausibility for each sample\n",
    "            impl_scores = []\n",
    "            for i in range(len(samples)):\n",
    "                predictions = [[pred_mean[i,j], pred_var[i,j]] \n",
    "                              for j in range(len(self.observed_data))]\n",
    "                observations = [[obs, 0.1] for obs in self.observed_data.values()]\n",
    "                \n",
    "                result = self.history_matcher.history_matching(observations, predictions)\n",
    "                impl_scores.append(result[\"I\"][0])\n",
    "                \n",
    "            return samples, np.array(impl_scores)\n",
    "        else:\n",
    "            # Run actual simulations\n",
    "            successful_samples = []\n",
    "            impl_scores = []\n",
    "            \n",
    "            for _, row in tqdm(samples.iterrows(), total=len(samples)):\n",
    "                params = row.to_dict()\n",
    "                outputs = self.run_simulation(params)\n",
    "                \n",
    "                if outputs is None:\n",
    "                    continue\n",
    "                    \n",
    "                # Prepare predictions and observations\n",
    "                predictions = [[outputs[signal], 0.01]  # Small fixed variance\n",
    "                              for signal in self.observed_data]\n",
    "                observations = [[obs, 0.1]  # Observation variance\n",
    "                              for obs in self.observed_data.values()]\n",
    "                \n",
    "                result = self.history_matcher.history_matching(observations, predictions)\n",
    "                impl_scores.append(result[\"I\"][0])\n",
    "                successful_samples.append(params)\n",
    "            \n",
    "            return pd.DataFrame(successful_samples), np.array(impl_scores)\n",
    "\n",
    "    def history_matching(self, n_waves: int = 3, n_samples_per_wave: int = 100, \n",
    "                        use_emulator: bool = False):\n",
    "        \"\"\"\n",
    "        Perform iterative history matching\n",
    "        Returns:\n",
    "            all_samples: DataFrame of all evaluated parameter sets\n",
    "            all_impl_scores: Array of corresponding implausibility scores\n",
    "        \"\"\"\n",
    "        current_samples = self.generate_samples(n_samples_per_wave)\n",
    "        all_samples = pd.DataFrame()\n",
    "        all_impl_scores = np.array([])\n",
    "        \n",
    "        for wave in range(n_waves):\n",
    "            print(f\"\\n=== Wave {wave + 1}/{n_waves} ===\")\n",
    "            \n",
    "            # Determine if we should use emulator (after first wave if enabled)\n",
    "            wave_use_emulator = use_emulator and (wave > 0) and self.emulator_trained\n",
    "            \n",
    "            # Run the wave\n",
    "            successful_samples, impl_scores = self.run_wave(\n",
    "                current_samples, \n",
    "                use_emulator=wave_use_emulator\n",
    "            )\n",
    "            \n",
    "            print(f\"Evaluated {len(successful_samples)} samples\")\n",
    "            print(f\"Min implausibility: {np.min(impl_scores):.2f}\")\n",
    "            print(f\"Max implausibility: {np.max(impl_scores):.2f}\")\n",
    "            \n",
    "            # Store results\n",
    "            all_samples = pd.concat([all_samples, successful_samples])\n",
    "            all_impl_scores = np.concatenate([all_impl_scores, impl_scores])\n",
    "            \n",
    "            # Identify Not Ruled Out Yet (NROY) points\n",
    "            nroy_mask = impl_scores <= self.history_matcher.threshold\n",
    "            nroy_samples = successful_samples[nroy_mask]\n",
    "            print(f\"NROY points: {len(nroy_samples)}\")\n",
    "            \n",
    "            # Train emulator after first wave if requested\n",
    "            if wave == 0 and use_emulator and len(successful_samples) > 10:\n",
    "                X_train = successful_samples[self.param_names].values\n",
    "                # Collect all outputs for training data\n",
    "                y_train = []\n",
    "                for _, row in successful_samples.iterrows():\n",
    "                    outputs = self.run_simulation(row.to_dict())\n",
    "                    if outputs is not None:\n",
    "                        y_train.append([outputs[signal] for signal in self.observed_data])\n",
    "                y_train = np.array(y_train)\n",
    "                self.train_emulator(X_train, y_train)\n",
    "            \n",
    "            # Generate new samples for next wave\n",
    "            if wave < n_waves - 1:\n",
    "                if len(nroy_samples) >= 2:\n",
    "                    # Sample from NROY space\n",
    "                    new_samples = self.history_matcher._sample_new_points(\n",
    "                        nroy_samples[self.param_names].values, \n",
    "                        n_samples_per_wave\n",
    "                    )\n",
    "                    current_samples = pd.DataFrame(new_samples, columns=self.param_names)\n",
    "                else:\n",
    "                    # Fall back to random sampling\n",
    "                    print(\"Not enough NROY points - using random sampling\")\n",
    "                    current_samples = self.generate_samples(n_samples_per_wave)\n",
    "        \n",
    "        return all_samples, all_impl_scores\n",
    "\n",
    "    def plot_results(self, samples: pd.DataFrame, impl_scores: np.ndarray):\n",
    "        \"\"\"Plot 2D parameter space with implausibility\"\"\"\n",
    "        if len(self.param_names) < 2:\n",
    "            return\n",
    "            \n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        sc = ax.scatter(samples[self.param_names[0]], \n",
    "                        samples[self.param_names[1]], \n",
    "                        c=impl_scores,\n",
    "                        cmap='viridis_r',\n",
    "                        vmin=0,\n",
    "                        vmax=self.history_matcher.threshold*1.5)\n",
    "        \n",
    "        ax.set_xlabel(self.param_names[0])\n",
    "        ax.set_ylabel(self.param_names[1])\n",
    "        ax.set_title(\"History Matching Results\")\n",
    "        plt.colorbar(sc, label='Implausibility')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define observed data (target values)\n",
    "    observed_signals = {\n",
    "        \"ao.P_min\": 80.0,    # Minimum aortic pressure (mmHg)\n",
    "        \"ao.P_max\": 120.0,   # Maximum aortic pressure (mmHg)\n",
    "    }\n",
    "\n",
    "    # Define parameter ranges to explore\n",
    "    param_ranges = {\n",
    "        \"lv.E_act\": (0.0, 20.0),  # LV active elastance\n",
    "        \"lv.v_ref\": (0.0, 20.0),  # LV reference volume\n",
    "        \"la.E_act\": (0.0, 10.5),  # LA active elastance\n",
    "    }\n",
    "\n",
    "    # Create history matcher instance\n",
    "    hm = NaghaviHistoryMatcher(\n",
    "        observed_data=observed_signals,\n",
    "        param_ranges=param_ranges,\n",
    "        implausibility_threshold=3.0\n",
    "    )\n",
    "\n",
    "    # Run history matching (with emulator after first wave)\n",
    "    all_samples, all_impl_scores = hm.history_matching(\n",
    "        n_waves=3,\n",
    "        n_samples_per_wave=50,\n",
    "        use_emulator=True\n",
    "    )\n",
    "\n",
    "    # Plot results\n",
    "    hm.plot_results(all_samples, all_impl_scores)\n",
    "\n",
    "    # Print best parameters\n",
    "    best_idx = np.argmin(all_impl_scores)\n",
    "    print(\"\\nBest parameters found:\")\n",
    "    print(all_samples.iloc[best_idx])\n",
    "    print(f\"Implausibility: {all_impl_scores[best_idx]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.history_matching import BaseSimulator\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "class NaghaviSimulator(BaseSimulator):\n",
    "    def __init__(self, observed_data: Dict[str, float], \n",
    "                 param_ranges: Dict[str, Tuple[float, float]],\n",
    "                 n_cycles: int = 40, dt: float = 0.001):\n",
    "        \"\"\"\n",
    "        Initialize the Naghavi model simulator\n",
    "        \n",
    "        Args:\n",
    "            observed_data: Dictionary of target values {signal_name: target_value}\n",
    "            param_ranges: Dictionary of parameter ranges {param_name: (min, max)}\n",
    "            n_cycles: Number of cardiac cycles to simulate\n",
    "            dt: Time step for simulation\n",
    "        \"\"\"\n",
    "        self.observed_data = observed_data\n",
    "        self.param_ranges = param_ranges\n",
    "        self._param_names = list(param_ranges.keys())\n",
    "        self._output_names = list(observed_data.keys())\n",
    "        self.n_cycles = n_cycles\n",
    "        self.dt = dt\n",
    "        \n",
    "        # Time setup dictionary\n",
    "        self.time_setup = {\n",
    "            \"name\": \"HistoryMatching\",\n",
    "            \"ncycles\": n_cycles,\n",
    "            \"tcycle\": 1.0,\n",
    "            \"dt\": dt,\n",
    "            \"export_min\": 1\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def param_names(self) -> List[str]:\n",
    "        return self._param_names\n",
    "    \n",
    "    @property\n",
    "    def output_names(self) -> List[str]:\n",
    "        return self._output_names\n",
    "\n",
    "    def run_simulation(self, params: Dict[str, float]) -> Optional[Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Run a single Naghavi model simulation with given parameters\n",
    "        Returns dictionary of output values {signal_name: value}\n",
    "        \"\"\"\n",
    "        # Create parameter object\n",
    "        parobj = NaghaviModelParameters()\n",
    "        \n",
    "        # Set parameters from input\n",
    "        for param_name, value in params.items():\n",
    "            if param_name == \"T\":\n",
    "                continue  # Handle cycle time separately\n",
    "            try:\n",
    "                obj, param = param_name.split('.')\n",
    "                parobj._set_comp(obj, [obj], **{param: value})\n",
    "            except Exception as e:\n",
    "                print(f\"Error setting parameter {param_name}: {e}\")\n",
    "                return None\n",
    "        \n",
    "        # Adjust cycle time if specified\n",
    "        t_cycle = params.get(\"T\", 1.0)\n",
    "        self.time_setup[\"tcycle\"] = t_cycle\n",
    "        \n",
    "        # Create and run model\n",
    "        try:\n",
    "            model = NaghaviModel(time_setup_dict=self.time_setup, parobj=parobj, suppress_printing=True)\n",
    "            solver = Solver(model=model)\n",
    "            solver.setup(suppress_output=True, optimize_secondary_sv=False, conv_cols=[\"p_ao\"], method='LSODA')\n",
    "            solver.solve()\n",
    "            \n",
    "            if not solver.converged:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Simulation error: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Process results - extract last cycle min/max values\n",
    "        raw_results = {}\n",
    "        tind_fin = np.arange(start=model.time_object.n_t-model.time_object.n_c,\n",
    "                            stop=model.time_object.n_t)\n",
    "        \n",
    "        for signal_name in self.observed_data.keys():\n",
    "            try:\n",
    "                component, _ = signal_name.split('.')\n",
    "                if component in model.components:\n",
    "                    raw_signal = model.components[component].P_i.values[tind_fin]\n",
    "                    \n",
    "                    if '_min' in signal_name:\n",
    "                        raw_results[signal_name] = np.min(raw_signal)\n",
    "                    elif '_max' in signal_name:\n",
    "                        raw_results[signal_name] = np.max(raw_signal)\n",
    "                else:\n",
    "                    print(f\"Component {component} not found in model\")\n",
    "                    return None\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting signal {signal_name}: {e}\")\n",
    "                return None\n",
    "                \n",
    "        return raw_results\n",
    "        \n",
    "    def generate_initial_samples(self, n_samples: int) -> List[Dict[str, float]]:\n",
    "        \"\"\"Generate initial parameter samples\"\"\"\n",
    "        samples = []\n",
    "        for _ in range(n_samples):\n",
    "            sample = {\n",
    "                name: np.random.uniform(low, high)\n",
    "                for name, (low, high) in self.param_ranges.items()\n",
    "            }\n",
    "            samples.append(sample)\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated samples: [{'lv.E_act': 16.44668164954106, 'lv.v_ref': 16.66484178008966, 'la.E_act': 6.995720230050086}, {'lv.E_act': 14.18638349984107, 'lv.v_ref': 10.437816913562871, 'la.E_act': 10.325782129765841}, {'lv.E_act': 17.763574970682868, 'lv.v_ref': 1.578728859610934, 'la.E_act': 3.5339965516239236}, {'lv.E_act': 15.89932610823169, 'lv.v_ref': 2.6418414947065827, 'la.E_act': 5.5868699195253955}, {'lv.E_act': 12.315681414273609, 'lv.v_ref': 7.427292824432879, 'la.E_act': 9.96684825466384}, {'lv.E_act': 3.6472320290328564, 'lv.v_ref': 5.094370690995536, 'la.E_act': 4.3348015076625135}, {'lv.E_act': 11.4864979277379, 'lv.v_ref': 3.4333636461118022, 'la.E_act': 5.499709041879557}, {'lv.E_act': 14.36072526147296, 'lv.v_ref': 15.513363604085004, 'la.E_act': 0.7606309141553744}, {'lv.E_act': 8.165161917186534, 'lv.v_ref': 14.24867749134581, 'la.E_act': 5.731504193443328}, {'lv.E_act': 14.58323729406169, 'lv.v_ref': 8.551639445597008, 'la.E_act': 1.5857123652577774}]\n",
      "\n",
      "=== Wave 1/20 ===\n",
      "Evaluated 50 samples\n",
      "Min implausibility: 55.88\n",
      "Max implausibility: 63.27\n",
      "NROY points: 0\n",
      "Training emulator...\n",
      "No NROY points - generating new random samples\n",
      "\n",
      "=== Wave 2/20 ===\n",
      "Evaluated 50 samples\n",
      "Min implausibility: 55.88\n",
      "Max implausibility: 63.27\n",
      "NROY points: 0\n",
      "No NROY points - generating new random samples\n",
      "\n",
      "=== Wave 3/20 ===\n",
      "Evaluated 50 samples\n",
      "Min implausibility: 55.88\n",
      "Max implausibility: 63.27\n",
      "NROY points: 0\n",
      "No NROY points - generating new random samples\n",
      "\n",
      "=== Wave 4/20 ===\n",
      "Evaluated 50 samples\n",
      "Min implausibility: 55.88\n",
      "Max implausibility: 63.27\n",
      "NROY points: 0\n",
      "No NROY points - generating new random samples\n",
      "\n",
      "=== Wave 5/20 ===\n",
      "Evaluated 50 samples\n",
      "Min implausibility: 55.88\n",
      "Max implausibility: 63.27\n",
      "NROY points: 0\n",
      "No NROY points - generating new random samples\n",
      "\n",
      "=== Wave 6/20 ===\n",
      "Evaluated 50 samples\n",
      "Min implausibility: 55.88\n",
      "Max implausibility: 63.27\n",
      "NROY points: 0\n",
      "No NROY points - generating new random samples\n",
      "\n",
      "=== Wave 7/20 ===\n",
      "Evaluated 50 samples\n",
      "Min implausibility: 55.88\n",
      "Max implausibility: 63.27\n",
      "NROY points: 0\n",
      "No NROY points - generating new random samples\n",
      "\n",
      "=== Wave 8/20 ===\n",
      "Evaluated 50 samples\n",
      "Min implausibility: 55.88\n",
      "Max implausibility: 63.27\n",
      "NROY points: 0\n",
      "No NROY points - generating new random samples\n",
      "\n",
      "=== Wave 9/20 ===\n",
      "Evaluated 50 samples\n",
      "Min implausibility: 55.88\n",
      "Max implausibility: 63.27\n",
      "NROY points: 0\n",
      "No NROY points - generating new random samples\n",
      "\n",
      "=== Wave 10/20 ===\n",
      "Evaluated 50 samples\n",
      "Min implausibility: 55.88\n",
      "Max implausibility: 63.27\n",
      "NROY points: 0\n",
      "No NROY points - generating new random samples\n",
      "\n",
      "=== Wave 11/20 ===\n",
      "Evaluated 50 samples\n",
      "Min implausibility: 55.88\n",
      "Max implausibility: 63.27\n",
      "NROY points: 0\n",
      "No NROY points - generating new random samples\n",
      "\n",
      "=== Wave 12/20 ===\n",
      "Evaluated 50 samples\n",
      "Min implausibility: 55.88\n",
      "Max implausibility: 63.27\n",
      "NROY points: 0\n",
      "No NROY points - generating new random samples\n",
      "\n",
      "=== Wave 13/20 ===\n",
      "Evaluated 50 samples\n",
      "Min implausibility: 55.88\n",
      "Max implausibility: 63.27\n",
      "NROY points: 0\n",
      "No NROY points - generating new random samples\n",
      "\n",
      "=== Wave 14/20 ===\n",
      "Evaluated 50 samples\n",
      "Min implausibility: 55.88\n",
      "Max implausibility: 63.27\n",
      "NROY points: 0\n",
      "No NROY points - generating new random samples\n",
      "\n",
      "=== Wave 15/20 ===\n",
      "Evaluated 50 samples\n",
      "Min implausibility: 55.88\n",
      "Max implausibility: 63.27\n",
      "NROY points: 0\n",
      "No NROY points - generating new random samples\n",
      "\n",
      "=== Wave 16/20 ===\n",
      "Evaluated 50 samples\n",
      "Min implausibility: 55.88\n",
      "Max implausibility: 63.27\n",
      "NROY points: 0\n",
      "No NROY points - generating new random samples\n",
      "\n",
      "=== Wave 17/20 ===\n",
      "Evaluated 50 samples\n",
      "Min implausibility: 55.88\n",
      "Max implausibility: 63.27\n",
      "NROY points: 0\n",
      "No NROY points - generating new random samples\n",
      "\n",
      "=== Wave 18/20 ===\n",
      "Evaluated 50 samples\n",
      "Min implausibility: 55.88\n",
      "Max implausibility: 63.27\n",
      "NROY points: 0\n",
      "No NROY points - generating new random samples\n",
      "\n",
      "=== Wave 19/20 ===\n",
      "Evaluated 50 samples\n",
      "Min implausibility: 55.88\n",
      "Max implausibility: 63.27\n",
      "NROY points: 0\n",
      "No NROY points - generating new random samples\n",
      "\n",
      "=== Wave 20/20 ===\n",
      "Evaluated 50 samples\n",
      "Min implausibility: 55.88\n",
      "Max implausibility: 63.27\n",
      "NROY points: 0\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Tuple\n",
    "import numpy as np\n",
    "from autoemulate.history_matching import HistoryMatcher, BaseSimulator\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define your parameter ranges\n",
    "    param_ranges = {\n",
    "        \"lv.E_act\": (0.0, 20.0),  # LV active elastance\n",
    "        \"lv.v_ref\": (0.0, 20.0),  # LV reference volume\n",
    "        \"la.E_act\": (0.0, 10.5),  # LA active elastance\n",
    "    }\n",
    "    \n",
    "    # Define observed data with means and variances\n",
    "    observations = {\n",
    "        \"ao.P_min\": (80.0, 0.1),    # (mean, variance) for minimum aortic pressure\n",
    "        \"ao.P_max\": (120.0, 0.1),    # (mean, variance) for maximum aortic pressure\n",
    "    }\n",
    "    \n",
    "    # Create simulator instance\n",
    "    simulator = NaghaviSimulator(\n",
    "        observed_data={k: v[0] for k, v in observations.items()},  # Extract just the means\n",
    "        param_ranges=param_ranges,\n",
    "        n_cycles=40,\n",
    "        dt=0.001\n",
    "    )\n",
    "    \n",
    "    # Test generating samples\n",
    "    samples = simulator.generate_initial_samples(10)\n",
    "    print(\"Generated samples:\", samples)\n",
    "\n",
    "    # Create history matcher\n",
    "    hm = HistoryMatcher(\n",
    "        simulator=simulator,\n",
    "        observations=observations,  # This needs both means and variances\n",
    "        threshold=3.0\n",
    "    )\n",
    "\n",
    "    # Run history matching\n",
    "    all_samples, all_impl_scores = hm.run_history_matching(\n",
    "        n_waves=20,\n",
    "        n_samples_per_wave=50,\n",
    "        use_emulator=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated samples: [{'lv.E_act': 0.01899583198384791, 'lv.v_ref': 0.11735388969659422, 'la.E_act': 0.06706428302622719}, {'lv.E_act': 19.967185364154712, 'lv.v_ref': 15.996656068495279, 'la.E_act': 4.439618144332525}, {'lv.E_act': 11.51809456326762, 'lv.v_ref': 4.4687766625614955, 'la.E_act': 1.848240373300514}, {'lv.E_act': 8.276746257955377, 'lv.v_ref': 17.266542542678337, 'la.E_act': 3.105007605202241}, {'lv.E_act': 10.013880594144693, 'lv.v_ref': 16.73534497245315, 'la.E_act': 4.872149131305782}, {'lv.E_act': 1.0469093980523247, 'lv.v_ref': 5.191459528131748, 'la.E_act': 2.8790671105333536}, {'lv.E_act': 16.173616756245195, 'lv.v_ref': 6.503155083370933, 'la.E_act': 6.083580939037274}, {'lv.E_act': 17.87032515680659, 'lv.v_ref': 8.781094061376358, 'la.E_act': 5.667161015058096}, {'lv.E_act': 5.46834756035627, 'lv.v_ref': 19.770861939010885, 'la.E_act': 0.4508716054023002}, {'lv.E_act': 7.394499630932798, 'lv.v_ref': 19.004816793829836, 'la.E_act': 6.753537751233991}]\n"
     ]
    }
   ],
   "source": [
    "from autoemulate.history_matching import BaseSimulator\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class NaghaviSimulator(BaseSimulator):\n",
    "    def __init__(self, observed_data: Dict[str, float], \n",
    "                 param_ranges: Dict[str, Tuple[float, float]],\n",
    "                 n_cycles: int = 40, dt: float = 0.001):\n",
    "        \"\"\"\n",
    "        Initialize the Naghavi model simulator\n",
    "        \n",
    "        Args:\n",
    "            observed_data: Dictionary of target values {signal_name: target_value}\n",
    "            param_ranges: Dictionary of parameter ranges {param_name: (min, max)}\n",
    "            n_cycles: Number of cardiac cycles to simulate\n",
    "            dt: Time step for simulation\n",
    "        \"\"\"\n",
    "        self.observed_data = observed_data\n",
    "        self.param_ranges = param_ranges\n",
    "        self._param_names = list(param_ranges.keys())\n",
    "        self._output_names = list(observed_data.keys())\n",
    "        self.n_cycles = n_cycles\n",
    "        self.dt = dt\n",
    "        \n",
    "        # Time setup dictionary\n",
    "        self.time_setup = {\n",
    "            \"name\": \"HistoryMatching\",\n",
    "            \"ncycles\": n_cycles,\n",
    "            \"tcycle\": 1.0,\n",
    "            \"dt\": dt,\n",
    "            \"export_min\": 1\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def param_names(self) -> List[str]:\n",
    "        return self._param_names\n",
    "    \n",
    "    @property\n",
    "    def output_names(self) -> List[str]:\n",
    "        return self._output_names\n",
    "\n",
    "    def run_simulation(self, params: Dict[str, float]) -> Optional[Dict[str, float]]:\n",
    "        \"\"\"Run simulation with given parameters\"\"\"\n",
    "        # Your existing simulation code here\n",
    "        pass\n",
    "        \n",
    "    def generate_initial_samples(self, n_samples: int) -> List[Dict[str, float]]:\n",
    "        \"\"\"Generate initial parameter samples\"\"\"\n",
    "        samples = []\n",
    "        for _ in range(n_samples):\n",
    "            sample = {\n",
    "                name: np.random.uniform(low, high)\n",
    "                for name, (low, high) in self.param_ranges.items()\n",
    "            }\n",
    "            samples.append(sample)\n",
    "        return samples\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define your parameter ranges\n",
    "    param_ranges = {\n",
    "        \"lv.E_act\": (0.0, 20.0),  # LV active elastance\n",
    "        \"lv.v_ref\": (0.0, 20.0),  # LV reference volume\n",
    "        \"la.E_act\": (0.0, 10.5),  # LA active elastance\n",
    "    }\n",
    "    \n",
    "    # Define observed data with means and variances\n",
    "    observations = {\n",
    "        \"ao.P_min\": (80.0, 0.1),    # (mean, variance) for minimum aortic pressure\n",
    "        \"ao.P_max\": (120.0, 0.1),    # (mean, variance) for maximum aortic pressure\n",
    "    }\n",
    "    \n",
    "    # Create simulator instance\n",
    "    simulator = NaghaviSimulator(\n",
    "        observed_data={k: v[0] for k, v in observations.items()},  # Extract just the means\n",
    "        param_ranges=param_ranges,\n",
    "        n_cycles=40,\n",
    "        dt=0.001\n",
    "    )\n",
    "    \n",
    "    # Test generating samples\n",
    "    samples = simulator.generate_initial_samples(10)\n",
    "    print(\"Generated samples:\", samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

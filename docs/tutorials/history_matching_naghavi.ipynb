{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a demo for a complete autoemulate pipeline for the Naghavi heart model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you have a simulation , put your simulator in a simulator object \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.simulations.naghavi_cardiac_ModularCirc import extract_parameter_ranges\n",
    "# Usage example:\n",
    "parameters_range = extract_parameter_ranges('/Users/mfamili/work/ModularCirc/Tutorials/Tutorial_03/Parameters_01.json')\n",
    "parameters_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from autoemulate.experimental_design import LatinHypercube\n",
    "\n",
    "\n",
    "# Generate Latin Hypercube samples\n",
    "N_samples = 100\n",
    "lhd = LatinHypercube(list(parameters_range.values()))\n",
    "sample_array = lhd.sample(N_samples)\n",
    "sample_df = pd.DataFrame(sample_array, columns=parameters_range.keys())\n",
    "\n",
    "print(\"Number of parameters:\", sample_df.shape[1], \"Number of samples from each parameter:\", sample_df.shape[0])\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.simulations.naghavi_cardiac_ModularCirc import NaghaviSimulator\n",
    "# Initialize simulator with specific outputs\n",
    "simulator = NaghaviSimulator(\n",
    "    parameters_range=parameters_range, \n",
    "    output_variables=['lv.P_i', 'lv.P_o'],  # Only the ones you're interested in\n",
    "    n_cycles=300, \n",
    "    dt=0.001,\n",
    ")\n",
    "\n",
    "# Run batch simulations with the samples generated in Cell 1\n",
    "results = simulator.run_batch_simulations(sample_df)\n",
    "\n",
    "# Convert results to DataFrame for analysis\n",
    "results_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output names:\", simulator.output_names)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test your simulator with our test function to make sure it ios compatible wih AutoEmulate pipelien "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need a test for the simulator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import autoemulate as ae\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from autoemulate.experimental_design import LatinHypercube\n",
    "from autoemulate.compare import AutoEmulate\n",
    "from autoemulate.plotting import _predict_with_optional_std\n",
    "\n",
    "preprocessing_methods = [{\"name\" : \"VAE\", \"params\" : {\"reduced_dim\": 2}},\n",
    "                         {\"name\" : \"PCA\", \"params\" : {\"reduced_dim\": 2}}]\n",
    "preprocessing_methods = [{\"name\" : \"PCA\", \"params\" : {\"reduced_dim\": 2}}]\n",
    "em = AutoEmulate()\n",
    "em.setup(sample_df, results, models=[\"gp\"], scale_output = True, reduce_dim_output=True, preprocessing_methods=preprocessing_methods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = em.compare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.summarise_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#em.plot_eval(model=best_model)\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3) Evaluate the emulator (on the test set)\n",
    "gp = em.get_model('GaussianProcess')\n",
    "em.evaluate(gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp\n",
    "gp_final = em.refit(gp)\n",
    "em.plot_eval(gp_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available methods:\", [method for method in dir(gp_final) if callable(getattr(gp_final, method))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now run hjistory matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.history_matching import HistoryMatcher\n",
    "# Define observed data with means and variances\n",
    "observations = {\n",
    "    'lv.P_i_min': (0.0, 0.1),   # Minimum of minimum LV pressure\n",
    "    'lv.P_i_max': (20.0, 0.1),   # Maximum of minimum LV pressure\n",
    "    'lv.P_i_mean': (15.0, 0.1),  # Mean of minimum LV pressure\n",
    "    'lv.P_i_range': (15.0, 0.5), # Range of minimum LV pressure\n",
    "    'lv.P_o_min': (1.0, 0.1),  # Minimum of maximum LV pressure\n",
    "    'lv.P_o_max': (13.0, 0.1),  # Maximum of maximum LV pressure\n",
    "    'lv.P_o_mean': (12.0, 0.1), # Mean of maximum LV pressure\n",
    "    'lv.P_o_range': (20.0, 0.5)  # Range of maximum LV pressure\n",
    "}\n",
    "    \n",
    "# Test generating samples\n",
    "samples = simulator.generate_initial_samples(10)\n",
    "print(\"Generated samples:\", samples)\n",
    "\n",
    "# Create history matcher\n",
    "hm = HistoryMatcher(\n",
    "    simulator=simulator,\n",
    "    observations=observations,  # This needs both means and variances\n",
    "    threshold=3.0\n",
    ")\n",
    "\n",
    "# Run history matching\n",
    "all_samples, all_impl_scores, emulator = hm.run_history_matching(\n",
    "    n_waves=20,\n",
    "    n_samples_per_wave=10,\n",
    "    use_emulator=True,\n",
    "    initial_emulator=gp_final,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.plot_eval(emulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.evaluate(emulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.history_matching_dashboard import HistoryMatchingDashboard\n",
    "dashboard = HistoryMatchingDashboard(\n",
    "    samples=all_samples,\n",
    "    impl_scores=all_impl_scores,\n",
    "    param_names=simulator.param_names,  # Use simulator.param_names instead of parameter_names\n",
    "    output_names=simulator.output_names  # Use simulator.output_names instead of output_names\n",
    ")\n",
    "dashboard.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

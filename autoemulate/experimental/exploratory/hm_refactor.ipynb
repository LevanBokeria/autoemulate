{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactored workflow example\n",
    "\n",
    "This notebook runs the same history matching workflow as in the integration tutorial but uses the refactored history matcher that is currently in experimental as well as the GP emulator in experimental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gpytorch\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# imports from main\n",
    "from autoemulate.compare import AutoEmulate\n",
    "from autoemulate.emulators.gaussian_process import constant_mean, rbf\n",
    "from autoemulate.experimental_design import LatinHypercube\n",
    "from autoemulate.history_matching import HistoryMatching as HMold\n",
    "from autoemulate.history_matching_dashboard import HistoryMatchingDashboard\n",
    "from autoemulate.simulations.naghavi_cardiac_ModularCirc import NaghaviSimulator, extract_parameter_ranges\n",
    "\n",
    "# imports from experimental\n",
    "from autoemulate.experimental.emulators.gaussian_process.exact import (\n",
    "    GaussianProcessExact,\n",
    ")\n",
    "from autoemulate.experimental.history_matching import HistoryMatching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate\n",
    "\n",
    "Set up the Simulator and generate data OR read a data file if have run this previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_range = extract_parameter_ranges(\n",
    "    '../../../docs/data/naghavi_model_parameters.json'\n",
    ")\n",
    "\n",
    "simulator = NaghaviSimulator(\n",
    "    parameters_range=parameters_range, \n",
    "    output_variables=['lv.P_i', 'lv.P_o'],  # Only the ones you're interested in\n",
    "    n_cycles=300, \n",
    "    dt=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_path = 'parameters.csv'\n",
    "results_path = 'simulator_results.csv'\n",
    "\n",
    "# Check if the results file already exists\n",
    "if not os.path.exists(results_path):\n",
    "    N_samples = 100\n",
    "    lhd = LatinHypercube(list(parameters_range.values()))\n",
    "    sample_array = lhd.sample(N_samples)\n",
    "    sample_df = pd.DataFrame(sample_array, columns=parameters_range.keys())\n",
    "    \n",
    "    # Run batch simulations with the samples generated in Cell 1\n",
    "    results = simulator.run_batch_simulations(sample_df)\n",
    "    # Convert results to DataFrame for analysis\n",
    "    results_df = pd.DataFrame(results)\n",
    "    # Save the results to a CSV file\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    sample_df.to_csv(inputs_path, index=False)\n",
    "else:\n",
    "    results_df = pd.read_csv(results_path)\n",
    "    sample_df = pd.read_csv(inputs_path)\n",
    "    # have to run simulator once to populate simulator.output_names\n",
    "    _ = simulator.sample_forward(sample_df.iloc[0])\n",
    "\n",
    "y = torch.from_numpy(results_df.to_numpy()).float()\n",
    "x = torch.from_numpy(sample_df.to_numpy()).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a GP\n",
    "\n",
    "(this should be done with AutoEmulate obviously)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_pytorch = GaussianProcessExact(\n",
    "        x,\n",
    "        y,\n",
    "        gpytorch.likelihoods.MultitaskGaussianLikelihood,\n",
    "        constant_mean,\n",
    "        rbf,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_pytorch.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define observed data with means and variances\n",
    "observations = {\n",
    "    'lv.P_i_min': (5.0, 0.1),   # Minimum of minimum LV pressure\n",
    "    'lv.P_i_max': (20.0, 0.1),   # Maximum of minimum LV pressure\n",
    "    'lv.P_i_mean': (10.0, 0.1),  # Mean of minimum LV pressure\n",
    "    'lv.P_i_range': (15.0, 0.5), # Range of minimum LV pressure\n",
    "    'lv.P_o_min': (1.0, 0.1),  # Minimum of maximum LV pressure\n",
    "    'lv.P_o_max': (13.0, 0.1),  # Maximum of maximum LV pressure\n",
    "    'lv.P_o_mean': (12.0, 0.1), # Mean of maximum LV pressure\n",
    "    'lv.P_o_range': (20.0, 0.5)  # Range of maximum LV pressure\n",
    "}\n",
    "\n",
    "# Create history matcher\n",
    "hm = HistoryMatching(\n",
    "    simulator=simulator,\n",
    "    observations=observations,\n",
    "    threshold=3.0\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History Matching involves:\n",
    "- sampling parameters\n",
    "- making predictions for those parameterings\n",
    "- evaluating implausability of predictions\n",
    "- identifying which of the paraneters are not ruled out yet (NROY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5718, 6.1953, 1.6543, 4.7003, 2.3911, 3.0088, 2.5649, 7.1212],\n",
       "         [0.5718, 6.1953, 1.6543, 4.7003, 2.3911, 3.0088, 2.5649, 7.1212],\n",
       "         [0.5718, 6.1953, 1.6543, 4.7003, 2.3911, 3.0088, 2.5649, 7.1212],\n",
       "         [0.5718, 6.1953, 1.6543, 4.7003, 2.3911, 3.0088, 2.5649, 7.1212],\n",
       "         [0.5718, 6.1953, 1.6543, 4.7003, 2.3911, 3.0088, 2.5649, 7.1212]]),\n",
       " tensor([], dtype=torch.int64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = hm.sample_params(5)\n",
    "pred_means, pred_vars, _ = hm.predict(x, emulator=gp_pytorch)\n",
    "implausability = hm.calculate_implausibility(pred_means, pred_vars)\n",
    "nroy_indices = hm.get_nroy(implausability)\n",
    "\n",
    "implausability, nroy_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can execture an iterative sample-predict-evaluate procedure with `HM.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "History Matching: 100%|██████████| 20/20 [00:00<00:00, 37.11wave/s, samples=20, failed=0, NROY=0, min_impl=0.57, max_impl=7.12]\n"
     ]
    }
   ],
   "source": [
    "emulator = hm.run(\n",
    "    n_waves=20,\n",
    "    n_samples_per_wave=20,\n",
    "    emulator_predict=True,\n",
    "    emulator=gp_pytorch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([400, 16]), torch.Size([400, 8]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hm.tested_params.shape, hm.impl_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard = HistoryMatchingDashboard(\n",
    "    samples=hm.tested_params,\n",
    "    impl_scores=hm.impl_scores,\n",
    "    param_names=simulator.param_names,  \n",
    "    output_names=simulator.output_names, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Compare implausability scores between original and new HM implementation (given the same GP predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa62d31856354cae9131de917fa01844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cross-validating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1: train an sklearn GP model\n",
    "preprocessing_methods = [{\"name\" : \"PCA\", \"params\" : {\"reduced_dim\": 2}}]\n",
    "em = AutoEmulate()\n",
    "em.setup(\n",
    "    sample_df, \n",
    "    y, \n",
    "    models=[\"gp\"], \n",
    "    scale_output = True, \n",
    "    reduce_dim_output=True, \n",
    "    preprocessing_methods=preprocessing_methods, \n",
    "    print_setup=False\n",
    ")\n",
    "\n",
    "best_model = em.compare()\n",
    "gp_sklearn = em.get_model('GaussianProcess')\n",
    "gp_final = em.refit(gp_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. generate samples and predictions\n",
    "samples = hm.sample_params(200)\n",
    "pred_means, pred_std = gp_final.predict(samples, return_std=True)\n",
    "pred_vars = pred_means**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: get implausability scores using original HM implementation\n",
    "hmold = HMold(\n",
    "    simulator=simulator,\n",
    "    observations=observations,\n",
    "    threshold=3.0\n",
    ")\n",
    "\n",
    "impl_old = hmold.calculate_implausibility(pred_means, pred_vars)[\"I\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: get implausability scores using new HM implementation\n",
    "impl_new = hm.calculate_implausibility(torch.from_numpy(pred_means), torch.from_numpy(pred_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5: check the results are the same\n",
    "torch.allclose(torch.from_numpy(impl_old), impl_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
